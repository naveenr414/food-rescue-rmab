{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS + Whittle Indices Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the performance of various algorithms to solve the joint matching + activity task, when the number of volunteers is large and structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "import argparse \n",
    "import sys\n",
    "import secrets\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr0/home/naveenr/miniconda3/envs/food/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from rmab.simulator import RMABSimulator, run_heterogenous_policy, get_discounted_reward\n",
    "from rmab.omniscient_policies import *\n",
    "from rmab.fr_dynamics import get_all_transitions, get_db_data \n",
    "from rmab.compute_whittle import arm_compute_whittle_multi_prob\n",
    "from rmab.mcts_policies import *\n",
    "from rmab.utils import get_save_path, delete_duplicate_results, create_prob_distro\n",
    "import resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_per_process_memory_fraction(0.5)\n",
    "torch.set_num_threads(1)\n",
    "resource.setrlimit(resource.RLIMIT_AS, (30 * 1024 * 1024 * 1024, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_jupyter = 'ipykernel' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_jupyter: \n",
    "    seed        = 42\n",
    "    n_arms      = 4\n",
    "    volunteers_per_arm = 1\n",
    "    budget      = 1\n",
    "    discount    = 0.9\n",
    "    alpha       = 3 \n",
    "    n_episodes  = 1\n",
    "    episode_len = 50 \n",
    "    n_epochs    = 1\n",
    "    save_with_date = False \n",
    "    TIME_PER_RUN = 0.01 * 1000\n",
    "    lamb = 0\n",
    "    prob_distro = 'uniform'\n",
    "    reward_type = \"max\"\n",
    "    reward_parameters = {'universe_size': 20, 'arm_set_low': 0, 'arm_set_high': 1}\n",
    "    policy_lr=5e-3\n",
    "    value_lr=1e-4\n",
    "    train_iterations = 30\n",
    "    test_iterations = 30\n",
    "    out_folder = 'iterative'\n",
    "else:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--n_arms',         '-N', help='num beneficiaries (arms)', type=int, default=2)\n",
    "    parser.add_argument('--volunteers_per_arm',         '-V', help='volunteers per arm', type=int, default=5)\n",
    "    parser.add_argument('--episode_len',    '-H', help='episode length', type=int, default=50)\n",
    "    parser.add_argument('--n_episodes',     '-T', help='num episodes', type=int, default=1)\n",
    "    parser.add_argument('--budget',         '-B', help='budget', type=int, default=3)\n",
    "    parser.add_argument('--n_epochs',       '-E', help='number of epochs (num_repeats)', type=int, default=1)\n",
    "    parser.add_argument('--discount',       '-d', help='discount factor', type=float, default=0.9)\n",
    "    parser.add_argument('--alpha',          '-a', help='alpha: for conf radius', type=float, default=3)\n",
    "    parser.add_argument('--lamb',          '-l', help='lambda for matching-engagement tradeoff', type=float, default=0.5)\n",
    "    parser.add_argument('--universe_size', help='For set cover, total num unvierse elems', type=int, default=10)\n",
    "    parser.add_argument('--arm_set_low', help='Least size of arm set, for set cover', type=float, default=3)\n",
    "    parser.add_argument('--arm_set_high', help='Largest size of arm set, for set cover', type=float, default=6)\n",
    "    parser.add_argument('--reward_type',          '-r', help='Which type of custom reward', type=str, default='set_cover')\n",
    "    parser.add_argument('--seed',           '-s', help='random seed', type=int, default=42)\n",
    "    parser.add_argument('--prob_distro',           '-p', help='which prob distro [uniform,uniform_small,uniform_large,normal]', type=str, default='uniform')\n",
    "    parser.add_argument('--time_per_run',      '-t', help='time per MCTS run', type=float, default=.01*1000)\n",
    "    parser.add_argument('--policy_lr', help='Learning Rate Policy', type=float, default=5e-3)\n",
    "    parser.add_argument('--value_lr', help='Learning Rate Value', type=float, default=1e-4)\n",
    "    parser.add_argument('--train_iterations', help='Number of MCTS train iterations', type=int, default=30)\n",
    "    parser.add_argument('--test_iterations', help='Number of MCTS test iterations', type=int, default=30)\n",
    "    parser.add_argument('--out_folder', help='Which folder to write results to', type=str, default='iterative')\n",
    "\n",
    "    parser.add_argument('--use_date', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    n_arms      = args.n_arms\n",
    "    volunteers_per_arm = args.volunteers_per_arm\n",
    "    budget      = args.budget\n",
    "    discount    = args.discount\n",
    "    alpha       = args.alpha \n",
    "    seed        = args.seed\n",
    "    n_episodes  = args.n_episodes\n",
    "    episode_len = args.episode_len\n",
    "    n_epochs    = args.n_epochs\n",
    "    lamb = args.lamb\n",
    "    save_with_date = args.use_date\n",
    "    TIME_PER_RUN = args.time_per_run\n",
    "    prob_distro = args.prob_distro\n",
    "    policy_lr = args.policy_lr \n",
    "    value_lr = args.value_lr \n",
    "    out_folder = args.out_folder\n",
    "    train_iterations = args.train_iterations \n",
    "    test_iterations = args.test_iterations \n",
    "    reward_type = args.reward_type\n",
    "    reward_parameters = {'universe_size': args.universe_size,\n",
    "                        'arm_set_low': args.arm_set_low, \n",
    "                        'arm_set_high': args.arm_set_high}\n",
    "\n",
    "save_name = secrets.token_hex(4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 2\n",
    "n_actions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_population_size = 100 \n",
    "all_transitions = get_all_transitions(all_population_size)\n",
    "all_transitions = np.zeros((100,2,2,2))\n",
    "for a in range(100):\n",
    "    all_transitions[a][:,:,0] = 1\n",
    "\n",
    "all_transitions[90,1,1,1] = 1\n",
    "all_transitions[90,1,1,0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prob_distro == \"food_rescue\":\n",
    "    probs_by_user = json.load(open(\"../../results/food_rescue/match_probs.json\",\"r\"))\n",
    "    donation_id_to_latlon, recipient_location_to_latlon, rescues_by_user, all_rescue_data, user_id_to_latlon = get_db_data()\n",
    "    probs_by_num = {}\n",
    "    for i in rescues_by_user:\n",
    "        if str(i) in probs_by_user and probs_by_user[str(i)] > 0:\n",
    "            if len(rescues_by_user[i]) not in probs_by_num:\n",
    "                probs_by_num[len(rescues_by_user[i])] = []\n",
    "            probs_by_num[len(rescues_by_user[i])].append(probs_by_user[str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    all_features = np.arange(all_population_size)\n",
    "    N = all_population_size*volunteers_per_arm\n",
    "    if reward_type == \"set_cover\":\n",
    "        if prob_distro == \"fixed\":\n",
    "            match_probabilities = []\n",
    "            set_sizes = [int(reward_parameters['arm_set_low']) for i in range(N)]\n",
    "            for i in range(N):\n",
    "                s = set() \n",
    "                while len(s) < set_sizes[i]:\n",
    "                    s.add(np.random.randint(0,reward_parameters['universe_size']))\n",
    "                match_probabilities.append(s)\n",
    "        else:\n",
    "            set_sizes = [np.random.poisson(int(reward_parameters['arm_set_low'])) for i in range(N)]\n",
    "            match_probabilities = [set([np.random.randint(0,reward_parameters['universe_size']) for _ in range(set_sizes[i])]) for i in range(N)]\n",
    "    elif prob_distro == \"food_rescue\":\n",
    "        match_probabilities = [np.random.choice(probs_by_num[(i+2*volunteers_per_arm)//volunteers_per_arm]) for i in range(N)] \n",
    "    else:\n",
    "        match_probabilities = [np.random.uniform(reward_parameters['arm_set_low'],reward_parameters['arm_set_high']) for i in range(N)]\n",
    "\n",
    "    simulator = RMABSimulator(all_population_size, all_features, all_transitions,\n",
    "                n_arms, volunteers_per_arm, episode_len, n_epochs, n_episodes, budget, discount,number_states=n_states, reward_style='custom',match_probability_list=match_probabilities,TIME_PER_RUN=TIME_PER_RUN)\n",
    "    simulator.reward_type = reward_type \n",
    "    simulator.reward_parameters = reward_parameters \n",
    "    return simulator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_seed(seed_list,policy,is_mcts=False,per_epoch_function=None,train_iterations=0,test_iterations=0,test_length=20):\n",
    "    memories = []\n",
    "    scores = {\n",
    "        'reward': [],\n",
    "        'time': [], \n",
    "        'match': [], \n",
    "        'active_rate': [],\n",
    "    }\n",
    "\n",
    "    for seed in seed_list:\n",
    "        simulator = create_environment(seed)\n",
    "\n",
    "        if is_mcts:\n",
    "            simulator.mcts_train_iterations = train_iterations\n",
    "            simulator.mcts_test_iterations = test_iterations\n",
    "            simulator.policy_lr = policy_lr\n",
    "            simulator.value_lr = value_lr\n",
    "\n",
    "        simulator.match_probability_list[[33,6,90,85]] = [1,1,1,1] \n",
    "\n",
    "        if is_mcts:\n",
    "            match, active_rate, memory = run_heterogenous_policy(simulator, n_episodes, n_epochs, discount,policy,seed,lamb=lamb,should_train=True,test_T=test_length,get_memory=True,per_epoch_function=per_epoch_function)\n",
    "        else:\n",
    "            match, active_rate = run_heterogenous_policy(simulator, n_episodes, n_epochs, discount,policy,seed,lamb=lamb,should_train=True,test_T=test_length,per_epoch_function=per_epoch_function)\n",
    "        time_whittle = simulator.time_taken\n",
    "        discounted_reward = get_discounted_reward(match,active_rate,discount,lamb)\n",
    "        scores['reward'].append(discounted_reward)\n",
    "        scores['time'].append(time_whittle)\n",
    "        scores['match'].append(np.mean(match))\n",
    "        scores['active_rate'].append(np.mean(active_rate))\n",
    "        if is_mcts:\n",
    "            memories.append(memory)\n",
    "\n",
    "    return scores, memories, simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results['parameters'] = {'seed'      : seed,\n",
    "        'n_arms'    : n_arms,\n",
    "        'volunteers_per_arm': volunteers_per_arm, \n",
    "        'budget'    : budget,\n",
    "        'discount'  : discount, \n",
    "        'alpha'     : alpha, \n",
    "        'n_episodes': n_episodes, \n",
    "        'episode_len': episode_len, \n",
    "        'n_epochs'  : n_epochs, \n",
    "        'lamb': lamb,\n",
    "        'time_per_run': TIME_PER_RUN, \n",
    "        'prob_distro': prob_distro, \n",
    "        'policy_lr': policy_lr, \n",
    "        'value_lr': value_lr, \n",
    "        'reward_type': reward_type, \n",
    "        'universe_size': reward_parameters['universe_size'],\n",
    "        'arm_set_low': reward_parameters['arm_set_low'], \n",
    "        'arm_set_high': reward_parameters['arm_set_high'],\n",
    "        } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list = [seed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! (0, 1) 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [33  6 90 85 13  7 20 88  9 74]\n",
      "Took 0.015816211700439453 time for inference and 0.0007436275482177734 time for training\n",
      "9.030356093361338\n"
     ]
    }
   ],
   "source": [
    "policy = greedy_policy\n",
    "name = \"greedy\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,test_length=n_episodes*episode_len)\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! (0, 1) 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [52 80 34 50 92 21  0 51 53 20]\n",
      "Took 0.00959467887878418 time for inference and 0.0006926059722900391 time for training\n",
      "153.91747796928897\n"
     ]
    }
   ],
   "source": [
    "policy = random_policy\n",
    "name = \"random\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,test_length=n_episodes*episode_len)\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! (0, 1) 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [52 80 34 50 92 21  0 51 53 20]\n",
      "Took 0.44142699241638184 time for inference and 0.4895350933074951 time for training\n",
      "102.12645330925488\n"
     ]
    }
   ],
   "source": [
    "policy = whittle_activity_policy\n",
    "name = \"whittle_activity\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,test_length=n_episodes*episode_len)\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [33  6 90 85]\n",
      "Took 0.09461021423339844 time for inference and 0.193434476852417 time for training\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "policy = whittle_policy\n",
    "name = \"linear_whittle\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,test_length=n_episodes*episode_len)\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prob_distro == \"fixed\":\n",
    "    match_probs = simulator.match_probability_list[simulator.agent_idx]\n",
    "    avg_combo_ratio = []\n",
    "    for i in combinations(match_probs,budget):\n",
    "        s = set()\n",
    "        for j in i:\n",
    "            s = s.union(j)\n",
    "        avg_combo_ratio.append(len(s)/sum([len(j) for j in i]))\n",
    "    results['ratio'] = np.mean(avg_combo_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! (0, 1) 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [52 80 34 50 92 21  0 51 53 20]\n",
      "Took 0.4681978225708008 time for inference and 0.48312997817993164 time for training\n",
      "162.36176226917988\n"
     ]
    }
   ],
   "source": [
    "policy = whittle_greedy_policy\n",
    "name = \"greedy_whittle\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,test_length=n_episodes*episode_len)\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [33  6 90 85]\n",
      "Took 0.005227565765380859 time for inference and 1.6159911155700684 time for training\n",
      "9.948462247926797\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 4:\n",
    "    policy = q_iteration_policy\n",
    "    per_epoch_function = q_iteration_custom_epoch()\n",
    "    name = \"optimal\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,per_epoch_function=per_epoch_function,test_length=n_episodes*episode_len)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [33  6 90 85]\n",
      "Took 0.19565296173095703 time for inference and 0.2404630184173584 time for training\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "policy = shapley_whittle_custom_policy \n",
    "name = \"shapley_whittle_custom\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,test_length=n_episodes*episode_len)\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [33  6 90 85]\n",
      "Took 2.2051634788513184 time for inference and 0.24950575828552246 time for training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.948462247926797"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = mcts_shapley_policy\n",
    "name = \"mcts_shapley\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,is_mcts=True,test_iterations=400,test_length=n_episodes*episode_len)\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "np.mean(rewards['reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! (0, 1) 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [58 53 90 86 52 57 37 15 59 23 30 75 39 20 96 48 17 16 25 55]\n",
      "On 0 of 50\n",
      "On 1 of 50\n",
      "On 2 of 50\n",
      "On 3 of 50\n",
      "On 4 of 50\n",
      "On 5 of 50\n",
      "On 6 of 50\n",
      "On 7 of 50\n",
      "On 8 of 50\n",
      "On 9 of 50\n",
      "On 10 of 50\n",
      "On 11 of 50\n",
      "On 12 of 50\n",
      "On 13 of 50\n",
      "On 14 of 50\n",
      "On 15 of 50\n",
      "On 16 of 50\n",
      "On 17 of 50\n",
      "On 18 of 50\n",
      "On 19 of 50\n",
      "On 20 of 50\n",
      "On 21 of 50\n",
      "On 22 of 50\n",
      "On 23 of 50\n",
      "On 24 of 50\n",
      "On 25 of 50\n",
      "On 26 of 50\n",
      "On 27 of 50\n",
      "On 28 of 50\n",
      "On 29 of 50\n",
      "On 30 of 50\n",
      "On 31 of 50\n",
      "On 32 of 50\n",
      "On 33 of 50\n",
      "On 34 of 50\n",
      "On 35 of 50\n",
      "On 36 of 50\n",
      "On 37 of 50\n",
      "On 38 of 50\n",
      "On 39 of 50\n",
      "On 40 of 50\n",
      "On 41 of 50\n",
      "On 42 of 50\n",
      "On 43 of 50\n",
      "On 44 of 50\n",
      "On 45 of 50\n",
      "On 46 of 50\n",
      "On 47 of 50\n",
      "On 48 of 50\n",
      "On 49 of 50\n",
      "Took 51.27006483078003 time for inference and 4.831877708435059 time for training\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 20:\n",
    "    policy = whittle_iterative_policy\n",
    "    name = \"iterative_whittle\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,test_length=n_episodes*episode_len)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    np.mean(rewards['reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! (0, 1) 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [58 53 90 86 52 57 37 15 59 23 30 75 39 20 96 48 17 16 25 55 85  9 18 77\n",
      " 99 66 84 95 78 49 74 69 76 70 73 83 24 45 97 63 19 54 82  5 56 67  1 89\n",
      " 21 88 65 12 35 28 42 68 44 38 81 79 29  3 31 60 51 98 50 41 14  8 26 13\n",
      " 91  0  2 46 64 93 43 36 61 22 47 92 33 11 71 72  6 27 40  4 32 94 34  7\n",
      " 80 10 62 87]\n",
      "Took 12.557528018951416 time for inference and 5.503240585327148 time for training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.466044041940122"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = greedy_whittle_iterative_policy\n",
    "name = \"greedy_iterative_whittle\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,test_iterations=400,test_length=n_episodes*episode_len)\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "np.mean(rewards['reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! (0, 1) 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [58 53 90 86 52 57 37 15 59 23 30 75 39 20 96]\n",
      "Took 17.049158573150635 time for inference and 3.2303199768066406 time for training\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 20:\n",
    "    policy = shapley_whittle_iterative_policy\n",
    "    name = \"shapley_iterative_whittle\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,test_iterations=400,test_length=n_episodes*episode_len)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    np.mean(rewards['reward'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = get_save_path(out_folder,save_name,seed,use_date=save_with_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_duplicate_results(out_folder,\"\",results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(results,open('../../results/'+save_path,'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
