{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi Synthetic Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the performance of various algorithms to solve the joint matching + activity task, when the number of volunteers is large and structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "import argparse \n",
    "import sys\n",
    "import secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr0/home/naveenr/miniconda3/envs/food/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from rmab.simulator import RMABSimulator\n",
    "from rmab.omniscient_policies import *\n",
    "from rmab.fr_dynamics import get_all_transitions\n",
    "from rmab.mcts_policies import *\n",
    "from rmab.utils import get_save_path, delete_duplicate_results, create_prob_distro\n",
    "import resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_per_process_memory_fraction(0.5)\n",
    "torch.set_num_threads(1)\n",
    "resource.setrlimit(resource.RLIMIT_AS, (30 * 1024 * 1024 * 1024, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_jupyter = 'ipykernel' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_jupyter: \n",
    "    seed        = 43\n",
    "    n_arms      = 2\n",
    "    volunteers_per_arm = 2\n",
    "    budget      = 2\n",
    "    discount    = 0.9\n",
    "    alpha       = 3 \n",
    "    n_episodes  = 1\n",
    "    episode_len = 20 \n",
    "    n_epochs    = 1\n",
    "    save_with_date = False \n",
    "    TIME_PER_RUN = 0.01 * 1000\n",
    "    lamb = 0.5\n",
    "    prob_distro = 'uniform'\n",
    "    policy_lr=5e-3\n",
    "    value_lr=1e-4\n",
    "    train_iterations = 30\n",
    "    test_iterations = 30\n",
    "    out_folder = 'mcts_exploration/mcts_exploration'\n",
    "else:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--n_arms',         '-N', help='num beneficiaries (arms)', type=int, default=2)\n",
    "    parser.add_argument('--volunteers_per_arm',         '-V', help='volunteers per arm', type=int, default=5)\n",
    "    parser.add_argument('--episode_len',    '-H', help='episode length', type=int, default=20)\n",
    "    parser.add_argument('--n_episodes',     '-T', help='num episodes', type=int, default=1)\n",
    "    parser.add_argument('--budget',         '-B', help='budget', type=int, default=3)\n",
    "    parser.add_argument('--n_epochs',       '-E', help='number of epochs (num_repeats)', type=int, default=3)\n",
    "    parser.add_argument('--discount',       '-d', help='discount factor', type=float, default=0.9)\n",
    "    parser.add_argument('--alpha',          '-a', help='alpha: for conf radius', type=float, default=3)\n",
    "    parser.add_argument('--lamb',          '-l', help='lambda for matching-engagement tradeoff', type=float, default=0.5)\n",
    "    parser.add_argument('--seed',           '-s', help='random seed', type=int, default=42)\n",
    "    parser.add_argument('--prob_distro',           '-p', help='which prob distro [uniform,uniform_small,uniform_large,normal]', type=str, default='uniform')\n",
    "    parser.add_argument('--time_per_run',      '-t', help='time per MCTS run', type=float, default=.01*1000)\n",
    "    parser.add_argument('--policy_lr', help='Learning Rate Policy', type=float, default=5e-3)\n",
    "    parser.add_argument('--value_lr', help='Learning Rate Value', type=float, default=1e-4)\n",
    "    parser.add_argument('--train_iterations', help='Number of MCTS train iterations', type=int, default=30)\n",
    "    parser.add_argument('--test_iterations', help='Number of MCTS test iterations', type=int, default=30)\n",
    "    parser.add_argument('--out_folder', help='Which folder to write results to', type=str, default='mcts_exploration/mcts_exploration')\n",
    "\n",
    "    parser.add_argument('--use_date', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    n_arms      = args.n_arms\n",
    "    volunteers_per_arm = args.volunteers_per_arm\n",
    "    budget      = args.budget\n",
    "    discount    = args.discount\n",
    "    alpha       = args.alpha \n",
    "    seed        = args.seed\n",
    "    n_episodes  = args.n_episodes\n",
    "    episode_len = args.episode_len\n",
    "    n_epochs    = args.n_epochs\n",
    "    lamb = args.lamb\n",
    "    save_with_date = args.use_date\n",
    "    TIME_PER_RUN = args.time_per_run\n",
    "    prob_distro = args.prob_distro\n",
    "    policy_lr = args.policy_lr \n",
    "    value_lr = args.value_lr \n",
    "    out_folder = args.out_folder\n",
    "    train_iterations = args.train_iterations \n",
    "    test_iterations = args.test_iterations \n",
    "\n",
    "save_name = secrets.token_hex(4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 2\n",
    "n_actions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_population_size = 100 # number of random arms to generate\n",
    "all_transitions = get_all_transitions(all_population_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if prob_distro == 'uniform':\n",
    "        match_probabilities = [np.random.random() for i in range(all_population_size * volunteers_per_arm)] \n",
    "    elif prob_distro == 'uniform_small':\n",
    "        match_probabilities = [np.random.random()/4 for i in range(all_population_size * volunteers_per_arm)] \n",
    "    elif prob_distro == 'uniform_large':\n",
    "        match_probabilities = [np.random.random()/4+0.75 for i in range(all_population_size * volunteers_per_arm)] \n",
    "    elif prob_distro == 'normal':\n",
    "        match_probabilities = [np.clip(np.random.normal(0.25, 0.1),0,1) for i in range(all_population_size * volunteers_per_arm)] \n",
    "\n",
    "\n",
    "    all_features = np.arange(all_population_size)\n",
    "    match_probabilities = create_prob_distro(prob_distro,all_population_size*volunteers_per_arm)\n",
    "\n",
    "    simulator = RMABSimulator(all_population_size, all_features, all_transitions,\n",
    "                n_arms, volunteers_per_arm, episode_len, n_epochs, n_episodes, budget, discount,number_states=n_states, reward_style='custom',match_probability_list=match_probabilities,TIME_PER_RUN=TIME_PER_RUN)\n",
    "\n",
    "    return simulator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_seed(seed_list,policy,is_mcts=False,per_epoch_function=None,train_iterations=0,test_iterations=0,test_length=20):\n",
    "    memories = []\n",
    "    scores = {\n",
    "        'reward': [],\n",
    "        'time': [], \n",
    "        'match': [], \n",
    "        'active_rate': [],\n",
    "    }\n",
    "\n",
    "    for seed in seed_list:\n",
    "        simulator = create_environment(seed)\n",
    "        if is_mcts:\n",
    "            simulator.mcts_train_iterations = train_iterations\n",
    "            simulator.mcts_test_iterations = test_iterations\n",
    "            simulator.policy_lr = policy_lr\n",
    "            simulator.value_lr = value_lr\n",
    "\n",
    "        if is_mcts:\n",
    "            match, active_rate, memory = run_heterogenous_policy(simulator, n_episodes, n_epochs, discount,policy,seed,lamb=lamb,should_train=True,test_T=test_length,get_memory=True,per_epoch_function=per_epoch_function)\n",
    "        else:\n",
    "            match, active_rate = run_heterogenous_policy(simulator, n_episodes, n_epochs, discount,policy,seed,lamb=lamb,should_train=True,test_T=test_length,per_epoch_function=per_epoch_function)\n",
    "        time_whittle = simulator.time_taken\n",
    "        discounted_reward = get_discounted_reward(match,active_rate,discount,lamb)\n",
    "        scores['reward'].append(discounted_reward)\n",
    "        scores['time'].append(time_whittle)\n",
    "        scores['match'].append(np.mean(match))\n",
    "        scores['active_rate'].append(np.mean(active_rate))\n",
    "        if is_mcts:\n",
    "            memories.append(memory)\n",
    "\n",
    "    return scores, memories, simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results['parameters'] = {'seed'      : seed,\n",
    "        'n_arms'    : n_arms,\n",
    "        'volunteers_per_arm': volunteers_per_arm, \n",
    "        'budget'    : budget,\n",
    "        'discount'  : discount, \n",
    "        'alpha'     : alpha, \n",
    "        'n_episodes': n_episodes, \n",
    "        'episode_len': episode_len, \n",
    "        'n_epochs'  : n_epochs, \n",
    "        'lamb': lamb,\n",
    "        'time_per_run': TIME_PER_RUN, \n",
    "        'prob_distro': prob_distro, \n",
    "        'policy_lr': policy_lr, \n",
    "        'value_lr': value_lr} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [35 47]\n",
      "Took 0.154388427734375 time for inference and 0.17846131324768066 time for training\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [43 47]\n",
      "Took 0.10646772384643555 time for inference and 0.13812017440795898 time for training\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [75  8]\n",
      "Took 0.13347268104553223 time for inference and 0.1335756778717041 time for training\n",
      "3.841497486703796\n"
     ]
    }
   ],
   "source": [
    "policy = whittle_policy\n",
    "name = \"linear_whittle\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,test_length=20)\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [35 47]\n",
      "Took 0.0019021034240722656 time for inference and 3.0827250480651855 time for training\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [43 47]\n",
      "Took 0.0018742084503173828 time for inference and 3.133578300476074 time for training\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [75  8]\n",
      "Took 0.0018856525421142578 time for inference and 2.701589822769165 time for training\n",
      "3.8915425725394983\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 4:\n",
    "    policy = q_iteration_policy\n",
    "    per_epoch_function = q_iteration_custom_epoch()\n",
    "    name = \"optimal\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,per_epoch_function=per_epoch_function)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [35 47]\n",
      "Took 0.14617443084716797 time for inference and 0.15169095993041992 time for training\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [43 47]\n",
      "Took 0.10081744194030762 time for inference and 0.15172576904296875 time for training\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [75  8]\n",
      "Took 0.13576221466064453 time for inference and 0.13466191291809082 time for training\n",
      "3.841497486703796\n"
     ]
    }
   ],
   "source": [
    "policy = shapley_whittle_policy \n",
    "name = \"shapley_whittle\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy)\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [35 47]\n",
      "Took 0.14826202392578125 time for inference and 0.15092825889587402 time for training\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [43 47]\n",
      "Took 0.10142230987548828 time for inference and 0.13527441024780273 time for training\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [75  8]\n",
      "Took 0.13315057754516602 time for inference and 0.14705538749694824 time for training\n",
      "3.7121314356102304\n"
     ]
    }
   ],
   "source": [
    "policy = shapley_whittle_submodular_policy \n",
    "name = \"shapley_whittle\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy)\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [35 47]\n",
      "State [1 1 0 1] Action [1 1 0 0]\n",
      "State [1 1 0 0] Action [1 1 0 0]\n",
      "State [1 1 0 0] Action [1 1 0 0]\n",
      "State [0 0 0 0] Action [1 0 1 0]\n",
      "State [0 0 1 0] Action [1 0 1 0]\n",
      "State [0 1 0 0] Action [0 1 1 0]\n",
      "State [0 1 0 0] Action [0 1 1 0]\n",
      "State [0 1 0 0] Action [0 1 1 0]\n",
      "State [1 1 0 0] Action [1 1 0 0]\n",
      "State [0 1 0 0] Action [0 1 1 0]\n",
      "State [1 1 1 0] Action [1 0 1 0]\n",
      "State [1 1 0 0] Action [1 1 0 0]\n",
      "State [1 1 0 0] Action [1 1 0 0]\n",
      "State [0 1 0 0] Action [0 1 1 0]\n",
      "State [0 0 0 0] Action [1 0 1 0]\n",
      "State [0 0 1 0] Action [1 0 1 0]\n",
      "State [1 0 1 0] Action [1 0 1 0]\n",
      "State [0 0 1 0] Action [1 0 1 0]\n",
      "State [0 0 1 0] Action [1 0 1 0]\n",
      "State [1 0 1 0] Action [1 0 1 0]\n",
      "Took 0.15401172637939453 time for inference and 0.15954923629760742 time for training\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [43 47]\n",
      "State [0 0 1 1] Action [0 1 1 0]\n",
      "State [0 0 1 0] Action [0 1 1 0]\n",
      "State [0 0 1 0] Action [0 1 1 0]\n",
      "State [0 0 1 0] Action [0 1 1 0]\n",
      "State [0 0 1 0] Action [0 1 1 0]\n",
      "State [0 1 1 0] Action [0 1 1 0]\n",
      "State [0 1 1 0] Action [0 1 1 0]\n",
      "State [0 0 1 0] Action [0 1 1 0]\n",
      "State [0 1 1 0] Action [0 1 1 0]\n",
      "State [0 1 1 0] Action [0 1 1 0]\n",
      "State [0 1 1 0] Action [0 1 1 0]\n",
      "State [0 1 1 0] Action [0 1 1 0]\n",
      "State [1 1 1 0] Action [0 1 1 0]\n",
      "State [1 1 1 0] Action [0 1 1 0]\n",
      "State [1 1 1 0] Action [0 1 1 0]\n",
      "State [0 1 1 0] Action [0 1 1 0]\n",
      "State [0 0 1 1] Action [0 1 1 0]\n",
      "State [0 1 1 0] Action [0 1 1 0]\n",
      "State [0 1 1 0] Action [0 1 1 0]\n",
      "State [1 1 1 0] Action [0 1 1 0]\n",
      "Took 0.10641336441040039 time for inference and 0.1424243450164795 time for training\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [75  8]\n",
      "State [0 1 0 1] Action [1 0 0 1]\n",
      "State [1 1 0 1] Action [0 1 0 1]\n",
      "State [1 1 0 1] Action [0 1 0 1]\n",
      "State [1 1 0 0] Action [0 1 0 1]\n",
      "State [0 1 0 0] Action [1 0 0 1]\n",
      "State [1 0 0 0] Action [0 1 0 1]\n",
      "State [0 1 0 0] Action [1 0 0 1]\n",
      "State [0 1 0 0] Action [1 0 0 1]\n",
      "State [1 1 0 0] Action [0 1 0 1]\n",
      "State [1 1 1 1] Action [0 0 1 1]\n",
      "State [1 1 0 0] Action [0 1 0 1]\n",
      "State [1 1 0 0] Action [0 1 0 1]\n",
      "State [1 1 0 0] Action [0 1 0 1]\n",
      "State [1 1 0 0] Action [0 1 0 1]\n",
      "State [0 1 0 0] Action [1 0 0 1]\n",
      "State [1 0 0 0] Action [0 1 0 1]\n",
      "State [1 1 0 1] Action [0 1 0 1]\n",
      "State [0 1 0 1] Action [1 0 0 1]\n",
      "State [1 1 0 0] Action [0 1 0 1]\n",
      "State [1 1 0 1] Action [0 1 0 1]\n",
      "Took 0.14021944999694824 time for inference and 0.13642072677612305 time for training\n",
      "3.841497486703796\n"
     ]
    }
   ],
   "source": [
    "policy = shapley_whittle_custom_policy \n",
    "name = \"shapley_whittle\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy)\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [35 47]\n",
      "State [1 1 0 1] Action [1 0 1 0]\n",
      "State [1 1 0 0] Action [1 0 1 0]\n",
      "State [1 1 0 0] Action [1 0 1 0]\n",
      "State [0 0 0 0] Action [1 0 1 0]\n",
      "State [0 0 1 0] Action [1 0 1 0]\n",
      "State [0 1 0 0] Action [0 1 1 0]\n",
      "State [0 1 0 0] Action [0 1 1 0]\n",
      "State [0 1 0 0] Action [0 1 1 0]\n",
      "State [1 1 0 0] Action [1 0 1 0]\n",
      "State [0 1 0 0] Action [0 1 1 0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmcts_shapley\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m results_by_iter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 6\u001b[0m rewards, memory, simulator \u001b[38;5;241m=\u001b[39m \u001b[43mrun_multi_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mis_mcts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mtest_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_reward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name)] \u001b[38;5;241m=\u001b[39m rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_match\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name)] \u001b[38;5;241m=\u001b[39m  rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch\u001b[39m\u001b[38;5;124m'\u001b[39m] \n",
      "Cell \u001b[0;32mIn[52], line 19\u001b[0m, in \u001b[0;36mrun_multi_seed\u001b[0;34m(seed_list, policy, is_mcts, per_epoch_function, train_iterations, test_iterations, test_length)\u001b[0m\n\u001b[1;32m     16\u001b[0m     simulator\u001b[38;5;241m.\u001b[39mvalue_lr \u001b[38;5;241m=\u001b[39m value_lr\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mcts:\n\u001b[0;32m---> 19\u001b[0m     match, active_rate, memory \u001b[38;5;241m=\u001b[39m \u001b[43mrun_heterogenous_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlamb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlamb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshould_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mtest_T\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43mget_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mper_epoch_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mper_epoch_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     match, active_rate \u001b[38;5;241m=\u001b[39m run_heterogenous_policy(simulator, n_episodes, n_epochs, discount,policy,seed,lamb\u001b[38;5;241m=\u001b[39mlamb,should_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,test_T\u001b[38;5;241m=\u001b[39mtest_length,per_epoch_function\u001b[38;5;241m=\u001b[39mper_epoch_function)\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/omniscient_policies.py:1084\u001b[0m, in \u001b[0;36mrun_heterogenous_policy\u001b[0;34m(env, n_episodes, n_epochs, discount, policy, seed, per_epoch_function, lamb, get_memory, should_train, test_T)\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1082\u001b[0m     all_active_rate[epoch,t] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(state)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(state)\n\u001b[0;32m-> 1084\u001b[0m action,memory \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbudget\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlamb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43mper_epoch_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m T: env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/mcts_policies.py:1087\u001b[0m, in \u001b[0;36mmcts_shapley_policy\u001b[0;34m(env, state, budget, lamb, memory, per_epoch_results, group_setup)\u001b[0m\n\u001b[1;32m   1085\u001b[0m s\u001b[38;5;241m.\u001b[39mper_epoch_results \u001b[38;5;241m=\u001b[39m per_epoch_results\n\u001b[1;32m   1086\u001b[0m root \u001b[38;5;241m=\u001b[39m MonteCarloTreeSearchNode(s,env\u001b[38;5;241m.\u001b[39mmcts_test_iterations,transitions\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mtransitions)\n\u001b[0;32m-> 1087\u001b[0m selected_idx \u001b[38;5;241m=\u001b[39m \u001b[43mroot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbudget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1088\u001b[0m memory \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mmemory \n\u001b[1;32m   1090\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(N, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint8)\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/mcts_policies.py:951\u001b[0m, in \u001b[0;36mMonteCarloTreeSearchNode.best_action\u001b[0;34m(self, budget)\u001b[0m\n\u001b[1;32m    949\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    950\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tree_policy()\n\u001b[0;32m--> 951\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    952\u001b[0m     v\u001b[38;5;241m.\u001b[39mbackpropagate(reward)\n\u001b[1;32m    953\u001b[0m choices_weights \u001b[38;5;241m=\u001b[39m [(c\u001b[38;5;241m.\u001b[39mq() \u001b[38;5;241m/\u001b[39m c\u001b[38;5;241m.\u001b[39mn(),c\u001b[38;5;241m.\u001b[39mn()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt((\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn()) \u001b[38;5;241m/\u001b[39m c\u001b[38;5;241m.\u001b[39mn())) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren]\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/mcts_policies.py:909\u001b[0m, in \u001b[0;36mMonteCarloTreeSearchNode.rollout\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_policy(possible_moves)\n\u001b[1;32m    908\u001b[0m     current_rollout_state \u001b[38;5;241m=\u001b[39m current_rollout_state\u001b[38;5;241m.\u001b[39mmove(action,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransitions)        \n\u001b[0;32m--> 909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcurrent_rollout_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/mcts_policies.py:1018\u001b[0m, in \u001b[0;36mStateAction.game_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     state_WI \u001b[38;5;241m=\u001b[39m whittle_index(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv,last_state,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbudget,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlamb,memory_whittle,reward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined\u001b[39m\u001b[38;5;124m\"\u001b[39m,match_probs\u001b[38;5;241m=\u001b[39mmemory_shapley)\n\u001b[1;32m   1017\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(state_WI[last_action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m-> 1018\u001b[0m     total_reward \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlamb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_state\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_action\u001b[49m\u001b[38;5;241m*\u001b[39mmemory_shapley)\n\u001b[1;32m   1019\u001b[0m     total_reward \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(last_state)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(last_state)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlamb\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_reward\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "policy = mcts_shapley_policy\n",
    "name = \"mcts_shapley\"\n",
    "\n",
    "results_by_iter = {}\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,test_length=20,is_mcts=True,test_iterations=iters)\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = get_save_path(out_folder,save_name,seed,use_date=save_with_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_duplicate_results(out_folder,\"\",results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(results,open('../../results/'+save_path,'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
