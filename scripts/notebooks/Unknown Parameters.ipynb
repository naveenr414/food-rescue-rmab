{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the performance of our Whittle and Adaptive Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "import argparse \n",
    "import sys\n",
    "import secrets\n",
    "from itertools import combinations\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr0/home/naveenr/projects/food_rescue_preferences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rmab.simulator import run_multi_seed\n",
    "from rmab.whittle_policies import *\n",
    "from rmab.baseline_policies import *\n",
    "from rmab.mcts_policies import *\n",
    "from rmab.occupancy_policies import *\n",
    "from rmab.utils import get_save_path, delete_duplicate_results, restrict_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_jupyter = 'ipykernel' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_jupyter: \n",
    "    seed        = 44\n",
    "    n_arms      = 10\n",
    "    volunteers_per_arm = 1\n",
    "    budget      = 2\n",
    "    discount    = 0.9\n",
    "    alpha       = 3 \n",
    "    n_episodes  = 30\n",
    "    episode_len = 50 \n",
    "    n_epochs    = 1\n",
    "    save_with_date = False \n",
    "    lamb = 0.5\n",
    "    prob_distro = 'uniform'\n",
    "    reward_type = \"probability_random\"\n",
    "    reward_parameters = {'universe_size': 20, 'arm_set_low': 0, 'arm_set_high': 1}\n",
    "    out_folder = 'unknown_parameters'\n",
    "    time_limit = 100\n",
    "else:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--n_arms',         '-N', help='num beneficiaries (arms)', type=int, default=2)\n",
    "    parser.add_argument('--volunteers_per_arm',         '-V', help='volunteers per arm', type=int, default=5)\n",
    "    parser.add_argument('--episode_len',    '-H', help='episode length', type=int, default=50)\n",
    "    parser.add_argument('--n_episodes',     '-T', help='num episodes', type=int, default=30)\n",
    "    parser.add_argument('--budget',         '-B', help='budget', type=int, default=3)\n",
    "    parser.add_argument('--n_epochs',       '-E', help='number of epochs (num_repeats)', type=int, default=1)\n",
    "    parser.add_argument('--discount',       '-d', help='discount factor', type=float, default=0.9)\n",
    "    parser.add_argument('--alpha',          '-a', help='alpha: for conf radius', type=float, default=3)\n",
    "    parser.add_argument('--lamb',          '-l', help='lambda for matching-engagement tradeoff', type=float, default=0.5)\n",
    "    parser.add_argument('--universe_size', help='For set cover, total num unvierse elems', type=int, default=10)\n",
    "    parser.add_argument('--arm_set_low', help='Least size of arm set, for set cover', type=float, default=3)\n",
    "    parser.add_argument('--arm_set_high', help='Largest size of arm set, for set cover', type=float, default=6)\n",
    "    parser.add_argument('--reward_type',          '-r', help='Which type of custom reward', type=str, default='set_cover')\n",
    "    parser.add_argument('--seed',           '-s', help='random seed', type=int, default=42)\n",
    "    parser.add_argument('--prob_distro',           '-p', help='which prob distro [uniform,uniform_small,uniform_large,normal]', type=str, default='uniform')\n",
    "    parser.add_argument('--out_folder', help='Which folder to write results to', type=str, default='iterative')\n",
    "    parser.add_argument('--time_limit', help='Online time limit for computation', type=float, default=100)\n",
    "    parser.add_argument('--use_date', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    n_arms      = args.n_arms\n",
    "    volunteers_per_arm = args.volunteers_per_arm\n",
    "    budget      = args.budget\n",
    "    discount    = args.discount\n",
    "    alpha       = args.alpha \n",
    "    seed        = args.seed\n",
    "    n_episodes  = args.n_episodes\n",
    "    episode_len = args.episode_len\n",
    "    n_epochs    = args.n_epochs\n",
    "    lamb = args.lamb\n",
    "    save_with_date = args.use_date\n",
    "    prob_distro = args.prob_distro\n",
    "    out_folder = args.out_folder\n",
    "    reward_type = args.reward_type\n",
    "    reward_parameters = {'universe_size': args.universe_size,\n",
    "                        'arm_set_low': args.arm_set_low, \n",
    "                        'arm_set_high': args.arm_set_high}\n",
    "    time_limit = args.time_limit \n",
    "    context_dim = n_arms*volunteers_per_arm\n",
    "\n",
    "save_name = secrets.token_hex(4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results['parameters'] = {'seed'      : seed,\n",
    "        'n_arms'    : n_arms,\n",
    "        'volunteers_per_arm': volunteers_per_arm, \n",
    "        'budget'    : budget,\n",
    "        'discount'  : discount, \n",
    "        'alpha'     : alpha, \n",
    "        'n_episodes': n_episodes, \n",
    "        'episode_len': episode_len, \n",
    "        'n_epochs'  : n_epochs, \n",
    "        'lamb': lamb,\n",
    "        'prob_distro': prob_distro, \n",
    "        'reward_type': reward_type, \n",
    "        'universe_size': reward_parameters['universe_size'],\n",
    "        'arm_set_low': reward_parameters['arm_set_low'], \n",
    "        'arm_set_high': reward_parameters['arm_set_high'],\n",
    "        'time_limit': time_limit, \n",
    "        } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupancy Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list = [seed]\n",
    "restrict_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [35  4 66  7 43 18 84 86 46 30]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "instance 0, ep 5\n",
      "instance 0, ep 6\n",
      "instance 0, ep 7\n",
      "instance 0, ep 8\n",
      "instance 0, ep 9\n",
      "instance 0, ep 10\n",
      "instance 0, ep 11\n",
      "instance 0, ep 12\n",
      "instance 0, ep 13\n",
      "instance 0, ep 14\n",
      "instance 0, ep 15\n",
      "instance 0, ep 16\n",
      "instance 0, ep 17\n",
      "instance 0, ep 18\n",
      "instance 0, ep 19\n",
      "instance 0, ep 20\n",
      "instance 0, ep 21\n",
      "instance 0, ep 22\n",
      "instance 0, ep 23\n",
      "instance 0, ep 24\n",
      "instance 0, ep 25\n",
      "instance 0, ep 26\n",
      "instance 0, ep 27\n",
      "instance 0, ep 28\n",
      "instance 0, ep 29\n",
      "Took 0.32500362396240234 time for inference and 0.0005028247833251953 time for training\n",
      "0.5015666666666669\n"
     ]
    }
   ],
   "source": [
    "policy = random_policy\n",
    "name = \"random\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [35  4 66  7 43 18 84 86 46 30]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "instance 0, ep 5\n",
      "instance 0, ep 6\n",
      "instance 0, ep 7\n",
      "instance 0, ep 8\n",
      "instance 0, ep 9\n",
      "instance 0, ep 10\n",
      "instance 0, ep 11\n",
      "instance 0, ep 12\n",
      "instance 0, ep 13\n",
      "instance 0, ep 14\n",
      "instance 0, ep 15\n",
      "instance 0, ep 16\n",
      "instance 0, ep 17\n",
      "instance 0, ep 18\n",
      "instance 0, ep 19\n",
      "instance 0, ep 20\n",
      "instance 0, ep 21\n",
      "instance 0, ep 22\n",
      "instance 0, ep 23\n",
      "instance 0, ep 24\n",
      "instance 0, ep 25\n",
      "instance 0, ep 26\n",
      "instance 0, ep 27\n",
      "instance 0, ep 28\n",
      "instance 0, ep 29\n",
      "Took 0.5840697288513184 time for inference and 0.0008356571197509766 time for training\n",
      "0.6294000000000005\n"
     ]
    }
   ],
   "source": [
    "policy = greedy_policy\n",
    "name = \"greedy\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_arms * volunteers_per_arm <= 4:\n",
    "    policy = q_iteration_policy\n",
    "    per_epoch_function = q_iteration_custom_epoch()\n",
    "    name = \"optimal\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],per_epoch_function=per_epoch_function,test_length=episode_len*(n_episodes%50))\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [35  4 66  7 43 18 84 86 46 30]\n",
      "Match probs [1, 1, 0, 1, 1, 1, 0, 0, 1, 0]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "instance 0, ep 5\n",
      "instance 0, ep 6\n",
      "instance 0, ep 7\n",
      "instance 0, ep 8\n",
      "instance 0, ep 9\n",
      "instance 0, ep 10\n",
      "instance 0, ep 11\n",
      "instance 0, ep 12\n",
      "instance 0, ep 13\n",
      "instance 0, ep 14\n",
      "instance 0, ep 15\n",
      "instance 0, ep 16\n",
      "instance 0, ep 17\n",
      "instance 0, ep 18\n",
      "instance 0, ep 19\n",
      "instance 0, ep 20\n",
      "instance 0, ep 21\n",
      "instance 0, ep 22\n",
      "instance 0, ep 23\n",
      "instance 0, ep 24\n",
      "instance 0, ep 25\n",
      "instance 0, ep 26\n",
      "instance 0, ep 27\n",
      "instance 0, ep 28\n",
      "instance 0, ep 29\n",
      "Took 0.4591069221496582 time for inference and 0.0008101463317871094 time for training\n",
      "0.6902666666666673\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 1000:\n",
    "    policy = whittle_policy \n",
    "    name = \"linear_whittle\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),shapley_iterations=1000)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [35  4 66  7 43 18 84 86 46 30]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "instance 0, ep 5\n",
      "instance 0, ep 6\n",
      "instance 0, ep 7\n",
      "instance 0, ep 8\n",
      "instance 0, ep 9\n",
      "instance 0, ep 10\n",
      "instance 0, ep 11\n",
      "instance 0, ep 12\n",
      "instance 0, ep 13\n",
      "instance 0, ep 14\n",
      "instance 0, ep 15\n",
      "instance 0, ep 16\n",
      "instance 0, ep 17\n",
      "instance 0, ep 18\n",
      "instance 0, ep 19\n",
      "instance 0, ep 20\n",
      "instance 0, ep 21\n",
      "instance 0, ep 22\n",
      "instance 0, ep 23\n",
      "instance 0, ep 24\n",
      "instance 0, ep 25\n",
      "instance 0, ep 26\n",
      "instance 0, ep 27\n",
      "instance 0, ep 28\n",
      "instance 0, ep 29\n",
      "Took 0.5031173229217529 time for inference and 0.10595512390136719 time for training\n",
      "0.7259333333333338\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 1000:\n",
    "    policy = shapley_whittle_custom_policy \n",
    "    name = \"shapley_whittle_custom\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),shapley_iterations=1000)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [35  4 66  7 43 18 84 86 46 30]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "instance 0, ep 5\n",
      "instance 0, ep 6\n",
      "instance 0, ep 7\n",
      "instance 0, ep 8\n",
      "instance 0, ep 9\n",
      "instance 0, ep 10\n",
      "instance 0, ep 11\n",
      "instance 0, ep 12\n",
      "instance 0, ep 13\n",
      "instance 0, ep 14\n",
      "instance 0, ep 15\n",
      "instance 0, ep 16\n",
      "instance 0, ep 17\n",
      "instance 0, ep 18\n",
      "instance 0, ep 19\n",
      "instance 0, ep 20\n",
      "instance 0, ep 21\n",
      "instance 0, ep 22\n",
      "instance 0, ep 23\n",
      "instance 0, ep 24\n",
      "instance 0, ep 25\n",
      "instance 0, ep 26\n",
      "instance 0, ep 27\n",
      "instance 0, ep 28\n",
      "instance 0, ep 29\n",
      "Took 0.5409672260284424 time for inference and 0.018103837966918945 time for training\n",
      "0.7170666666666671\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 1000:\n",
    "    policy = regression_policy \n",
    "    name = \"regression_whittle\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),shapley_iterations=1000)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [35  4 66  7 43 18 84 86 46 30]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "instance 0, ep 5\n",
      "instance 0, ep 6\n",
      "instance 0, ep 7\n",
      "instance 0, ep 8\n",
      "instance 0, ep 9\n",
      "instance 0, ep 10\n",
      "instance 0, ep 11\n",
      "instance 0, ep 12\n",
      "instance 0, ep 13\n",
      "instance 0, ep 14\n",
      "instance 0, ep 15\n",
      "instance 0, ep 16\n",
      "instance 0, ep 17\n",
      "instance 0, ep 18\n",
      "instance 0, ep 19\n",
      "instance 0, ep 20\n",
      "instance 0, ep 21\n",
      "instance 0, ep 22\n",
      "instance 0, ep 23\n",
      "instance 0, ep 24\n",
      "instance 0, ep 25\n",
      "instance 0, ep 26\n",
      "instance 0, ep 27\n",
      "instance 0, ep 28\n",
      "instance 0, ep 29\n",
      "Took 1.9588074684143066 time for inference and 0.07530379295349121 time for training\n",
      "0.7017333333333339\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 1000:\n",
    "    policy = occupancy_measure_linear \n",
    "    name = \"occupancy_measure_linear\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),shapley_iterations=1000)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [35  4 66  7 43 18 84 86 46 30]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "instance 0, ep 5\n",
      "instance 0, ep 6\n",
      "instance 0, ep 7\n",
      "instance 0, ep 8\n",
      "instance 0, ep 9\n",
      "instance 0, ep 10\n",
      "instance 0, ep 11\n",
      "instance 0, ep 12\n",
      "instance 0, ep 13\n",
      "instance 0, ep 14\n",
      "instance 0, ep 15\n",
      "instance 0, ep 16\n",
      "instance 0, ep 17\n",
      "instance 0, ep 18\n",
      "instance 0, ep 19\n",
      "instance 0, ep 20\n",
      "instance 0, ep 21\n",
      "instance 0, ep 22\n",
      "instance 0, ep 23\n",
      "instance 0, ep 24\n",
      "instance 0, ep 25\n",
      "instance 0, ep 26\n",
      "instance 0, ep 27\n",
      "instance 0, ep 28\n",
      "instance 0, ep 29\n",
      "Took 1.8553857803344727 time for inference and 0.1530752182006836 time for training\n",
      "0.7402000000000005\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 1000:\n",
    "    policy = occupancy_measure_shapley \n",
    "    name = \"occupancy_measure_shapley\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),shapley_iterations=1000)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [35  4 66  7 43 18 84 86 46 30]\n",
      "instance 0, ep 1\n",
      "Mem [ 0.88904611  0.27178239  0.44676149  0.34977821  0.60088716  0.07789384\n",
      " -0.13209419 -0.07789384  0.26418838  0.52369349]\n",
      "Real [ 0.77192982  0.21637427  0.32748538  0.21637427  0.43859649  0.21637427\n",
      " -0.00584795  0.21637427  0.43859649  0.21637427]\n",
      "50 0.16282172324378275\n",
      "instance 0, ep 2\n",
      "Mem [ 0.69932358  0.33583884  0.45825317  0.38590275  0.45638899 -0.01037511\n",
      " -0.14336681  0.01037511  0.28673363  0.33588297]\n",
      "Real [0.69005848 0.46783626 0.24561404 0.35672515 0.69005848 0.24561404\n",
      " 0.13450292 0.24561404 0.46783626 0.24561404]\n",
      "100 0.11106784030358287\n",
      "instance 0, ep 3\n",
      "Mem [ 0.64687953  0.3616872   0.37813561  0.39134808  0.43460769  0.01384818\n",
      " -0.15188541 -0.01384818  0.30377081  0.37581096]\n",
      "Real [0.73099415 0.39766082 0.1754386  0.39766082 0.50877193 0.06432749\n",
      " 0.06432749 0.06432749 0.61988304 0.39766082]\n",
      "150 0.1203252906639935\n",
      "instance 0, ep 4\n",
      "Mem [ 0.7098444   0.35932212  0.31513077  0.43333178  0.3906035  -0.03022302\n",
      " -0.13999251  0.03022302  0.27998502  0.29956144]\n",
      "Real [0.67251462 0.4502924  0.56140351 0.33918129 0.4502924  0.4502924\n",
      " 0.11695906 0.11695906 0.67251462 0.11695906]\n",
      "200 0.12400885777619583\n",
      "instance 0, ep 5\n",
      "Mem [ 0.70681154  0.36133583  0.31780979  0.39447085  0.37788053 -0.03256365\n",
      " -0.14006942  0.03256365  0.28013885  0.29480328]\n",
      "Real [0.73684211 0.40350877 0.29239766 0.07017544 0.40350877 0.29239766\n",
      " 0.29239766 0.07017544 0.62573099 0.18128655]\n",
      "250 0.12336773717854338\n",
      "instance 0, ep 6\n",
      "Mem [ 0.72301843  0.3037369   0.30793781  0.35261385  0.34765075 -0.04207703\n",
      " -0.13721162  0.04207703  0.27442323  0.27863433]\n",
      "Real [0.70175439 0.25730994 0.25730994 0.14619883 0.36842105 0.70175439\n",
      " 0.14619883 0.36842105 0.59064327 0.14619883]\n",
      "300 0.08546236470813737\n",
      "instance 0, ep 7\n",
      "Mem [ 0.71856855  0.3038661   0.31061081  0.33856125  0.34448936 -0.02631857\n",
      " -0.16474504  0.02631857  0.32949008  0.28261782]\n",
      "Real [0.57894737 0.35672515 0.46783626 0.02339181 0.69005848 0.46783626\n",
      " 0.13450292 0.46783626 0.57894737 0.02339181]\n",
      "350 0.24437808033557962\n",
      "instance 0, ep 8\n",
      "Mem [ 0.70999709  0.30311014  0.31690867  0.34427574  0.35094078 -0.02693929\n",
      " -0.15714901  0.02693929  0.31429803  0.28897241]\n",
      "Real [ 0.71345029  0.38011696  0.49122807  0.26900585  0.49122807  0.60233918\n",
      " -0.06432749 -0.06432749  0.49122807  0.26900585]\n",
      "400 0.10689017938299454\n",
      "instance 0, ep 9\n",
      "Mem [ 0.69632269  0.30187155  0.32641153  0.35333469  0.36174375 -0.01724685\n",
      " -0.16620015  0.01724685  0.33240031  0.29930615]\n",
      "Real [0.7251462  0.05847953 0.16959064 0.50292398 0.7251462  0.61403509\n",
      " 0.05847953 0.16959064 0.28070175 0.16959064]\n",
      "450 0.07911431591350633\n",
      "instance 0, ep 10\n",
      "Mem [ 0.67837218  0.30017471  0.33969036  0.36522499  0.37594737 -0.00533441\n",
      " -0.17648311  0.00533441  0.35296623  0.31284807]\n",
      "Real [ 0.71345029  0.38011696  0.49122807  0.38011696  0.38011696  0.60233918\n",
      " -0.1754386   0.04678363  0.60233918  0.15789474]\n",
      "500 0.14222553586784054\n",
      "instance 0, ep 11\n",
      "Mem [ 0.67477892  0.29983578  0.34234804  0.36760172  0.37883316 -0.00414797\n",
      " -0.17617038  0.00414797  0.35234076  0.31553369]\n",
      "Real [ 0.80116959  0.35672515  0.02339181  0.24561404  0.57894737  0.24561404\n",
      "  0.13450292 -0.0877193   0.46783626  0.02339181]\n",
      "550 0.12094308403448861\n",
      "instance 0, ep 12\n",
      "Mem [ 0.67142584  0.29955739  0.34483748  0.36986631  0.38105059 -0.00266824\n",
      " -0.17659905  0.00266824  0.35319811  0.31806446]\n",
      "Real [ 0.73099415  0.50877193  0.28654971  0.39766082  0.39766082  0.39766082\n",
      " -0.04678363  0.06432749  0.50877193  0.1754386 ]\n",
      "600 0.0772507867911636\n",
      "instance 0, ep 13\n",
      "Mem [ 0.65501559  0.29800384  0.35697622  0.38073272  0.39407386  0.00798092\n",
      " -0.18552359 -0.00798092  0.37104718  0.33043826]\n",
      "Real [0.70760234 0.37426901 0.04093567 0.48538012 0.70760234 0.37426901\n",
      " 0.15204678 0.04093567 0.48538012 0.26315789]\n",
      "650 0.08345984123068809\n",
      "instance 0, ep 14\n",
      "Mem [ 0.65681785  0.29817543  0.35564257  0.37953503  0.39269699  0.00529465\n",
      " -0.18154181 -0.00529465  0.36308361  0.3290475 ]\n",
      "Real [ 0.71345029  0.49122807  0.15789474  0.38011696  0.49122807  0.38011696\n",
      " -0.06432749  0.15789474  0.49122807  0.38011696]\n",
      "700 0.09238845029099504\n",
      "instance 0, ep 15\n",
      "Mem [ 0.65201791  0.29775576  0.35920148  0.38275449  0.39609146  0.00825986\n",
      " -0.18384075 -0.00825986  0.3676815   0.33267896]\n",
      "Real [0.68421053 0.35087719 0.23976608 0.57309942 0.57309942 0.57309942\n",
      " 0.01754386 0.12865497 0.23976608 0.4619883 ]\n",
      "750 0.11237199897753429\n",
      "instance 0, ep 16\n",
      "Mem [ 0.61880877  0.29466443  0.38377957  0.40481017  0.42177847  0.03051618\n",
      " -0.20327454 -0.03051618  0.40654907  0.35775782]\n",
      "Real [ 0.71345029  0.15789474  0.38011696  0.49122807  0.71345029  0.49122807\n",
      " -0.06432749  0.04678363  0.38011696  0.26900585]\n",
      "800 0.13758181906829528\n",
      "instance 0, ep 17\n",
      "Mem [ 0.60903127  0.29373518  0.39101072  0.41127705  0.42961977  0.03597777\n",
      " -0.20684557 -0.03597777  0.41369115  0.36510996]\n",
      "Real [ 0.71929825  0.38596491 -0.05847953  0.2748538   0.60818713  0.2748538\n",
      "  0.05263158  0.38596491  0.60818713  0.2748538 ]\n",
      "850 0.15238148013035846\n",
      "instance 0, ep 18\n",
      "Mem [ 0.6079713   0.29363415  0.39179478  0.41197924  0.4304557   0.03698195\n",
      " -0.20804827 -0.03698195  0.41609653  0.36591563]\n",
      "Real [0.67836257 0.45614035 0.01169591 0.56725146 0.23391813 0.67836257\n",
      " 0.12280702 0.45614035 0.56725146 0.12280702]\n",
      "900 0.11077310006863211\n",
      "instance 0, ep 19\n",
      "Mem [ 0.59848515  0.29273205  0.39881071  0.4182555   0.43803732  0.0430395\n",
      " -0.21301442 -0.0430395   0.42602885  0.37306459]\n",
      "Real [ 0.76608187 -0.01169591 -0.01169591  0.32163743  0.43274854  0.43274854\n",
      " -0.01169591  0.32163743  0.54385965  0.32163743]\n",
      "950 0.14271376485485018\n",
      "instance 0, ep 20\n",
      "Mem [ 0.59511141  0.29241146  0.4013058   0.42048672  0.44074528  0.04485679\n",
      " -0.21411354 -0.04485679  0.42822707  0.37560004]\n",
      "Real [0.70175439 0.25730994 0.25730994 0.36842105 0.59064327 0.47953216\n",
      " 0.14619883 0.14619883 0.36842105 0.36842105]\n",
      "1000 0.0832244975428652\n",
      "instance 0, ep 21\n",
      "Mem [ 0.58634279  0.29160201  0.4077965   0.42631409  0.44749803  0.04950776\n",
      " -0.21681664 -0.04950776  0.43363329  0.38219888]\n",
      "Real [0.68421053 0.35087719 0.4619883  0.4619883  0.57309942 0.4619883\n",
      " 0.23976608 0.12865497 0.23976608 0.23976608]\n",
      "1050 0.13911210858458806\n",
      "instance 0, ep 22\n",
      "Mem [ 0.57761044  0.29077072  0.41425448  0.43208888  0.454508    0.05461319\n",
      " -0.22045702 -0.05461319  0.44091403  0.38876936]\n",
      "Real [ 0.75438596  0.0877193   0.19883041  0.42105263  0.64327485  0.53216374\n",
      " -0.02339181  0.0877193   0.53216374 -0.02339181]\n",
      "1100 0.13401261774701204\n",
      "instance 0, ep 23\n",
      "Mem [ 0.57329238  0.29038146  0.41745302  0.43496949  0.45772298  0.05683508\n",
      " -0.22164857 -0.05683508  0.44329714  0.39202159]\n",
      "Real [0.71929825 0.16374269 0.2748538  0.16374269 0.49707602 0.71929825\n",
      " 0.05263158 0.16374269 0.60818713 0.16374269]\n",
      "1150 0.11674963339339227\n",
      "instance 0, ep 24\n",
      "Mem [ 0.57576177  0.29061711  0.41562678  0.43333604  0.45574698  0.05508337\n",
      " -0.22000948 -0.05508337  0.44001897  0.39015726]\n",
      "Real [0.64327485 0.30994152 0.42105263 0.42105263 0.53216374 0.53216374\n",
      " 0.19883041 0.42105263 0.53216374 0.19883041]\n",
      "1200 0.07982893162898977\n",
      "instance 0, ep 25\n",
      "Mem [ 0.57066661  0.29015212  0.41939984  0.43673013  0.45958688  0.05822758\n",
      " -0.22245179 -0.05822758  0.44490358  0.39400338]\n",
      "Real [0.70760234 0.15204678 0.04093567 0.37426901 0.48538012 0.70760234\n",
      " 0.15204678 0.37426901 0.48538012 0.15204678]\n",
      "1250 0.0677351642930235\n",
      "instance 0, ep 26\n",
      "Mem [ 0.56221608  0.28934505  0.42564896  0.44231692  0.46638434  0.06358472\n",
      " -0.22679994 -0.06358472  0.45359987  0.4003695 ]\n",
      "Real [0.70175439 0.25730994 0.36842105 0.36842105 0.59064327 0.47953216\n",
      " 0.25730994 0.25730994 0.36842105 0.03508772]\n",
      "1300 0.11235856044206824\n",
      "instance 0, ep 27\n",
      "Mem [ 0.55792716  0.28893574  0.42882048  0.44515138  0.46984723  0.06591636\n",
      " -0.22824029 -0.06591636  0.45648057  0.40359243]\n",
      "Real [ 0.54385965  0.43274854  0.21052632  0.43274854  0.65497076  0.54385965\n",
      " -0.01169591  0.32163743  0.43274854  0.54385965]\n",
      "1350 0.018899771239875685\n",
      "instance 0, ep 28\n",
      "Mem [ 0.5617599   0.28930188  0.42598615  0.44261716  0.46676833  0.06336446\n",
      " -0.22602641 -0.06336446  0.45205282  0.40070252]\n",
      "Real [ 0.7251462   0.39181287  0.50292398  0.28070175  0.50292398  0.39181287\n",
      " -0.05263158  0.16959064  0.39181287  0.16959064]\n",
      "1400 0.11181312809597821\n",
      "instance 0, ep 29\n",
      "Mem [ 0.56060185  0.2891913   0.42684252  0.44338271  0.46770065  0.06407466\n",
      " -0.2265749  -0.06407466  0.4531498   0.40157442]\n",
      "Real [0.71345029 0.26900585 0.26900585 0.26900585 0.60233918 0.38011696\n",
      " 0.15789474 0.15789474 0.60233918 0.15789474]\n",
      "1450 0.15101891525123376\n",
      "Took 6.890403509140015 time for inference and 0.00026726722717285156 time for training\n",
      "0.7193666666666672\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 1000:\n",
    "    policy = occupancy_measure_learn_shapley \n",
    "    name = \"occupancy_measure_learn_shapley\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),shapley_iterations=1000)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    results['{}_predicted_regression'.format(name)] = list(simulator.predicted_regression)\n",
    "    results['{}_actual_regression'.format(name)] = list(simulator.actual_regression)\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [35  4 66  7 43 18 84 86 46 30]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "instance 0, ep 5\n",
      "instance 0, ep 6\n",
      "instance 0, ep 7\n",
      "instance 0, ep 8\n",
      "instance 0, ep 9\n",
      "instance 0, ep 10\n",
      "instance 0, ep 11\n",
      "instance 0, ep 12\n",
      "instance 0, ep 13\n",
      "instance 0, ep 14\n",
      "instance 0, ep 15\n",
      "instance 0, ep 16\n",
      "instance 0, ep 17\n",
      "instance 0, ep 18\n",
      "instance 0, ep 19\n",
      "instance 0, ep 20\n",
      "instance 0, ep 21\n",
      "instance 0, ep 22\n",
      "instance 0, ep 23\n",
      "instance 0, ep 24\n",
      "instance 0, ep 25\n",
      "instance 0, ep 26\n",
      "instance 0, ep 27\n",
      "instance 0, ep 28\n",
      "instance 0, ep 29\n",
      "Took 3.7401883602142334 time for inference and 0.0002644062042236328 time for training\n",
      "0.7193666666666672\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 1000:\n",
    "    policy = occupancy_measure_learn_shapley_confidence\n",
    "    name = \"occupancy_measure_learn_shapley\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),shapley_iterations=1000)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    results['{}_predicted_regression'.format(name)] = list(simulator.predicted_regression)\n",
    "    results['{}_actual_regression'.format(name)] = list(simulator.actual_regression)\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = get_save_path(out_folder,save_name,seed,use_date=save_with_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_duplicate_results(out_folder,\"\",results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(results,open('../../results/'+save_path,'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
