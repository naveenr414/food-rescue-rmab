{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the performance of our Whittle and Adaptive Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "import argparse \n",
    "import sys\n",
    "import secrets\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rmab.whittle_policies import *\n",
    "from rmab.baseline_policies import *\n",
    "from rmab.mcts_policies import *\n",
    "from rmab.utils import get_save_path, delete_duplicate_results, restrict_resources\n",
    "from rmab.simulator import RMABSimulator, run_heterogenous_policy, get_discounted_reward, create_random_transitions\n",
    "from rmab.fr_dynamics import get_all_transitions, get_db_data, get_all_transitions_partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_jupyter = 'ipykernel' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_jupyter: \n",
    "    seed        = 43\n",
    "    n_arms      = 20\n",
    "    volunteers_per_arm = 1\n",
    "    budget      = 10\n",
    "    discount    = 0.9\n",
    "    alpha       = 3 \n",
    "    n_episodes  = 105\n",
    "    episode_len = 50 \n",
    "    n_epochs    = 1\n",
    "    save_with_date = False \n",
    "    lamb = 0.5\n",
    "    prob_distro = 'food_rescue_top'\n",
    "    reward_type = \"probability\"\n",
    "    reward_parameters = {'universe_size': 20, 'arm_set_low': 0, 'arm_set_high': 1}\n",
    "    out_folder = 'iterative'\n",
    "    time_limit = 100\n",
    "else:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--n_arms',         '-N', help='num beneficiaries (arms)', type=int, default=2)\n",
    "    parser.add_argument('--volunteers_per_arm',         '-V', help='volunteers per arm', type=int, default=5)\n",
    "    parser.add_argument('--episode_len',    '-H', help='episode length', type=int, default=50)\n",
    "    parser.add_argument('--n_episodes',     '-T', help='num episodes', type=int, default=105)\n",
    "    parser.add_argument('--budget',         '-B', help='budget', type=int, default=3)\n",
    "    parser.add_argument('--n_epochs',       '-E', help='number of epochs (num_repeats)', type=int, default=1)\n",
    "    parser.add_argument('--discount',       '-d', help='discount factor', type=float, default=0.9)\n",
    "    parser.add_argument('--alpha',          '-a', help='alpha: for conf radius', type=float, default=3)\n",
    "    parser.add_argument('--lamb',          '-l', help='lambda for matching-engagement tradeoff', type=float, default=0.5)\n",
    "    parser.add_argument('--universe_size', help='For set cover, total num unvierse elems', type=int, default=10)\n",
    "    parser.add_argument('--arm_set_low', help='Least size of arm set, for set cover', type=float, default=3)\n",
    "    parser.add_argument('--arm_set_high', help='Largest size of arm set, for set cover', type=float, default=6)\n",
    "    parser.add_argument('--reward_type',          '-r', help='Which type of custom reward', type=str, default='set_cover')\n",
    "    parser.add_argument('--seed',           '-s', help='random seed', type=int, default=42)\n",
    "    parser.add_argument('--prob_distro',           '-p', help='which prob distro [uniform,uniform_small,uniform_large,normal]', type=str, default='uniform')\n",
    "    parser.add_argument('--out_folder', help='Which folder to write results to', type=str, default='iterative')\n",
    "    parser.add_argument('--time_limit', help='Online time limit for computation', type=float, default=100)\n",
    "    parser.add_argument('--use_date', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    n_arms      = args.n_arms\n",
    "    volunteers_per_arm = args.volunteers_per_arm\n",
    "    budget      = args.budget\n",
    "    discount    = args.discount\n",
    "    alpha       = args.alpha \n",
    "    seed        = args.seed\n",
    "    n_episodes  = args.n_episodes\n",
    "    episode_len = args.episode_len\n",
    "    n_epochs    = args.n_epochs\n",
    "    lamb = args.lamb\n",
    "    save_with_date = args.use_date\n",
    "    prob_distro = args.prob_distro\n",
    "    out_folder = args.out_folder\n",
    "    reward_type = args.reward_type\n",
    "    reward_parameters = {'universe_size': args.universe_size,\n",
    "                        'arm_set_low': args.arm_set_low, \n",
    "                        'arm_set_high': args.arm_set_high}\n",
    "    time_limit = args.time_limit \n",
    "\n",
    "save_name = secrets.token_hex(4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results['parameters'] = {'seed'      : seed,\n",
    "        'n_arms'    : n_arms,\n",
    "        'volunteers_per_arm': volunteers_per_arm, \n",
    "        'budget'    : budget,\n",
    "        'discount'  : discount, \n",
    "        'alpha'     : alpha, \n",
    "        'n_episodes': n_episodes, \n",
    "        'episode_len': episode_len, \n",
    "        'n_epochs'  : n_epochs, \n",
    "        'lamb': lamb,\n",
    "        'prob_distro': prob_distro, \n",
    "        'reward_type': reward_type, \n",
    "        'universe_size': reward_parameters['universe_size'],\n",
    "        'arm_set_low': reward_parameters['arm_set_low'], \n",
    "        'arm_set_high': reward_parameters['arm_set_high'],\n",
    "        'time_limit': time_limit, \n",
    "        } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 2\n",
    "n_actions = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "all_population_size = 100 \n",
    "max_transition_prob = 0.25\n",
    "all_transitions = create_random_transitions(all_population_size,max_transition_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_volunteers(probs_by_num,num_by_section):\n",
    "    total = sum([len(probs_by_num[i]) for i in probs_by_num])\n",
    "    num_per_section = total//num_by_section\n",
    "\n",
    "    nums_by_partition = []\n",
    "    current_count = 0\n",
    "    current_partition = []\n",
    "\n",
    "    keys = sorted(probs_by_num.keys())\n",
    "\n",
    "    for i in keys:\n",
    "        if current_count >= num_per_section*(len(nums_by_partition)+1):\n",
    "            nums_by_partition.append(current_partition)\n",
    "            current_partition = []\n",
    "        \n",
    "        current_partition.append(i)\n",
    "        current_count += len(probs_by_num[i])\n",
    "    return nums_by_partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prob_distro == \"food_rescue_top\":\n",
    "    all_population_size = 20 \n",
    "    probs_by_user = json.load(open(\"../../../results/food_rescue/match_probs.json\",\"r\"))\n",
    "    donation_id_to_latlon, recipient_location_to_latlon, rescues_by_user, all_rescue_data, user_id_to_latlon = get_db_data()\n",
    "    probs_by_num = {}\n",
    "    for i in rescues_by_user:\n",
    "        if str(i) in probs_by_user and probs_by_user[str(i)] > 0 and len(rescues_by_user[i]) >= 100:\n",
    "            if len(rescues_by_user[i]) not in probs_by_num:\n",
    "                probs_by_num[len(rescues_by_user[i])] = []\n",
    "            probs_by_num[len(rescues_by_user[i])].append(probs_by_user[str(i)])\n",
    "\n",
    "    partitions = partition_volunteers(probs_by_num,all_population_size)\n",
    "    probs_by_partition = []\n",
    "\n",
    "    for i in range(len(partitions)):\n",
    "        temp_probs = []\n",
    "        for j in partitions[i]:\n",
    "            temp_probs += (probs_by_num[j])\n",
    "        probs_by_partition.append(temp_probs)\n",
    "\n",
    "    all_transitions = get_all_transitions_partition(all_population_size,partitions)\n",
    "\n",
    "    for i,partition in enumerate(partitions):\n",
    "        current_transitions = np.array(all_transitions[i])\n",
    "        partition_scale = np.array([len(probs_by_num[j]) for j in partition])\n",
    "        partition_scale = partition_scale/np.sum(partition_scale)\n",
    "        prod = current_transitions*partition_scale[:,np.newaxis,np.newaxis,np.newaxis]\n",
    "        new_transition = np.sum(prod,axis=0)\n",
    "        all_transitions[i] = new_transition\n",
    "    all_transitions = np.array(all_transitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    all_features = np.arange(all_population_size)\n",
    "    N = all_population_size*volunteers_per_arm\n",
    "    print(probs_by_partition[0])\n",
    "    match_probabilities = [np.random.choice(probs_by_partition[i//volunteers_per_arm]) for i in range(N)] \n",
    "\n",
    "    simulator = RMABSimulator(all_population_size, all_features, all_transitions,\n",
    "                n_arms, volunteers_per_arm, episode_len, n_epochs, n_episodes, budget, discount,number_states=n_states, reward_style='custom',match_probability_list=match_probabilities)\n",
    "    simulator.reward_type = reward_type \n",
    "    simulator.reward_parameters = reward_parameters \n",
    "    return simulator \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_seed(seed_list,policy,is_mcts=False,per_epoch_function=None,train_iterations=0,test_iterations=400,test_length=20,shapley_iterations=1000):\n",
    "    memories = []\n",
    "    scores = {\n",
    "        'reward': [],\n",
    "        'time': [], \n",
    "        'match': [], \n",
    "        'active_rate': [],\n",
    "    }\n",
    "\n",
    "    for seed in seed_list:\n",
    "        simulator = create_environment(seed)\n",
    "        simulator.time_limit = time_limit\n",
    "\n",
    "        simulator.mcts_train_iterations = train_iterations\n",
    "        simulator.mcts_test_iterations = test_iterations\n",
    "        simulator.shapley_iterations = shapley_iterations \n",
    "\n",
    "        if prob_distro == \"linearity\":\n",
    "            set_list = []\n",
    "\n",
    "            for i in range(budget):\n",
    "                set_list.append(set(list(range(1,int(reward_parameters['arm_set_high'])+1))))\n",
    "            \n",
    "            for i in range(n_arms*volunteers_per_arm-budget):\n",
    "                nums = list(range(1,int(reward_parameters['universe_size'])+1))[i*int(reward_parameters['arm_set_low']):(i+1)*int(reward_parameters['arm_set_low']):]\n",
    "                set_list.append(set(nums))\n",
    "            \n",
    "            simulator.match_probability_list[simulator.cohort_selection[0]]  = set_list \n",
    "\n",
    "        if prob_distro == \"one_time\":\n",
    "            N = n_arms*volunteers_per_arm\n",
    "            simulator.first_init_states = np.array([[[1 for i in range(N)] for i in range(n_episodes)]])\n",
    "            random.seed(seed)\n",
    "            shuffled_list = [reward_parameters['arm_set_high'] for i in range(2)] + [reward_parameters['arm_set_high'] for i in range(N-2)]\n",
    "            random.shuffle(shuffled_list)\n",
    "\n",
    "            simulator.match_probability_list[simulator.cohort_selection[0]] = shuffled_list\n",
    "\n",
    "        if is_mcts:\n",
    "            match, active_rate, memory = run_heterogenous_policy(simulator, n_episodes, n_epochs, discount,policy,seed,lamb=lamb,should_train=False,test_T=test_length,get_memory=True,per_epoch_function=per_epoch_function)\n",
    "        else:\n",
    "            match, active_rate = run_heterogenous_policy(simulator, n_episodes, n_epochs, discount,policy,seed,lamb=lamb,should_train=False,test_T=test_length,per_epoch_function=per_epoch_function)\n",
    "\n",
    "        num_timesteps = match.size\n",
    "        match = match.reshape((num_timesteps//episode_len,episode_len))\n",
    "        active_rate = active_rate.reshape((num_timesteps//episode_len,episode_len))\n",
    "\n",
    "        time_whittle = simulator.time_taken\n",
    "        discounted_reward = get_discounted_reward(match,active_rate,discount,lamb)\n",
    "        scores['reward'].append(discounted_reward)\n",
    "        scores['time'].append(time_whittle)\n",
    "        scores['match'].append(np.mean(match))\n",
    "        scores['active_rate'].append(np.mean(active_rate))\n",
    "        if is_mcts:\n",
    "            memories.append(memory)\n",
    "\n",
    "    return scores, memories, simulator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list = [seed]\n",
    "restrict_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001668307168715904, 0.0025209872185948017, 0.0035241048773611504, 0.004228866240960799, 0.05889281507656066, 0.09711538461538462, 0.018603794437281267, 0.010603674540682414, 0.040383846461415435, 0.0014444253761226474, 0.021632041122295994, 0.019370924434215574, 0.006340636574800678, 0.0035861383326232068, 0.06408629441624365, 0.01527063804052011, 0.056361607142857144, 0.001932184125781398, 0.006313834726090994, 0.015042029199233151, 0.062461726883037354, 0.013845527351703543, 0.022972972972972974, 0.0194954128440367, 0.0017701548455840651, 0.0036838340486409155, 0.0065755873340143005, 0.002111824165009329, 0.00638640873015873, 0.03698384201077199]\n",
      "cohort [ 6 19 10  7  1  9 12 16 17  8 14  0 13  4  5  3 18 11  2 15]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "instance 0, ep 5\n",
      "instance 0, ep 6\n",
      "instance 0, ep 7\n",
      "instance 0, ep 8\n",
      "instance 0, ep 9\n",
      "instance 0, ep 10\n",
      "instance 0, ep 11\n",
      "instance 0, ep 12\n",
      "instance 0, ep 13\n",
      "instance 0, ep 14\n",
      "instance 0, ep 15\n",
      "instance 0, ep 16\n",
      "instance 0, ep 17\n",
      "instance 0, ep 18\n",
      "instance 0, ep 19\n",
      "instance 0, ep 20\n",
      "instance 0, ep 21\n",
      "instance 0, ep 22\n",
      "instance 0, ep 23\n",
      "instance 0, ep 24\n",
      "instance 0, ep 25\n",
      "instance 0, ep 26\n",
      "instance 0, ep 27\n",
      "instance 0, ep 28\n",
      "instance 0, ep 29\n",
      "instance 0, ep 30\n",
      "instance 0, ep 31\n",
      "instance 0, ep 32\n",
      "instance 0, ep 33\n",
      "instance 0, ep 34\n",
      "instance 0, ep 35\n",
      "instance 0, ep 36\n",
      "instance 0, ep 37\n",
      "instance 0, ep 38\n",
      "instance 0, ep 39\n",
      "instance 0, ep 40\n",
      "instance 0, ep 41\n",
      "instance 0, ep 42\n",
      "instance 0, ep 43\n",
      "instance 0, ep 44\n",
      "instance 0, ep 45\n",
      "instance 0, ep 46\n",
      "instance 0, ep 47\n",
      "instance 0, ep 48\n",
      "instance 0, ep 49\n",
      "instance 0, ep 50\n",
      "instance 0, ep 51\n",
      "instance 0, ep 52\n",
      "instance 0, ep 53\n",
      "instance 0, ep 54\n",
      "instance 0, ep 55\n",
      "instance 0, ep 56\n",
      "instance 0, ep 57\n",
      "instance 0, ep 58\n",
      "instance 0, ep 59\n",
      "instance 0, ep 60\n",
      "instance 0, ep 61\n",
      "instance 0, ep 62\n",
      "instance 0, ep 63\n",
      "instance 0, ep 64\n",
      "instance 0, ep 65\n",
      "instance 0, ep 66\n",
      "instance 0, ep 67\n",
      "instance 0, ep 68\n",
      "instance 0, ep 69\n",
      "instance 0, ep 70\n",
      "instance 0, ep 71\n",
      "instance 0, ep 72\n",
      "instance 0, ep 73\n",
      "instance 0, ep 74\n",
      "instance 0, ep 75\n",
      "instance 0, ep 76\n",
      "instance 0, ep 77\n",
      "instance 0, ep 78\n",
      "instance 0, ep 79\n",
      "instance 0, ep 80\n",
      "instance 0, ep 81\n",
      "instance 0, ep 82\n",
      "instance 0, ep 83\n",
      "instance 0, ep 84\n",
      "instance 0, ep 85\n",
      "instance 0, ep 86\n",
      "instance 0, ep 87\n",
      "instance 0, ep 88\n",
      "instance 0, ep 89\n",
      "instance 0, ep 90\n",
      "instance 0, ep 91\n",
      "instance 0, ep 92\n",
      "instance 0, ep 93\n",
      "instance 0, ep 94\n",
      "instance 0, ep 95\n",
      "instance 0, ep 96\n",
      "instance 0, ep 97\n",
      "instance 0, ep 98\n",
      "instance 0, ep 99\n",
      "instance 0, ep 100\n",
      "Match probs 1.16623102531825\n",
      "Match probs 1.3231448654213749\n",
      "Match probs 0.952390728722182\n",
      "Match probs 0.941512245226726\n",
      "Match probs 1.0747819331054527\n",
      "Match probs 1.0872671147465984\n",
      "Match probs 1.0251469423387387\n",
      "Match probs 1.0143920678987723\n",
      "Match probs 0.9181516602317555\n",
      "Match probs 0.37460275995103776\n",
      "Match probs 0.768711606131128\n",
      "Match probs 1.2005131435076144\n",
      "Match probs 1.1254486052659525\n",
      "Match probs 1.138392971099755\n",
      "Match probs 0.4081970851045839\n",
      "Match probs 0.828338942072908\n",
      "Match probs 0.995263907420725\n",
      "Match probs 1.22363550582712\n",
      "Match probs 1.2369213565964605\n",
      "Match probs 1.0563449059096917\n",
      "Match probs 0.6568665853213326\n",
      "Match probs 0.7124337434319756\n",
      "Match probs 1.3042438684130844\n",
      "Match probs 1.3042438684130844\n",
      "Match probs 1.0667785866602135\n",
      "Match probs 1.0588895616807552\n",
      "Match probs 1.0987576341597327\n",
      "Match probs 0.7332595409648748\n",
      "Match probs 1.1650886223660464\n",
      "Match probs 1.054945786034131\n",
      "Match probs 1.054945786034131\n",
      "Match probs 1.1273134968835206\n",
      "Match probs 1.0917017704349123\n",
      "Match probs 1.0917017704349123\n",
      "Match probs 1.1671077936121124\n",
      "Match probs 1.0917017704349123\n",
      "Match probs 1.138392971099755\n",
      "Match probs 1.1229493513525872\n",
      "Match probs 1.109691044607932\n",
      "Match probs 1.2601129701782445\n",
      "Match probs 1.309316320776987\n",
      "Match probs 1.1254615012864868\n",
      "Match probs 1.2913270466039672\n",
      "Match probs 0.8462396584581405\n",
      "Match probs 0.878284640745305\n",
      "Match probs 0.7792069213190218\n",
      "Match probs 0.6420433024933644\n",
      "Match probs 1.0871582346638764\n",
      "Match probs 1.0871582346638764\n",
      "Match probs 1.0742138688300737\n",
      "instance 0, ep 101\n",
      "Match probs 0.3718182732300909\n",
      "Match probs 0.7846037471089039\n",
      "Match probs 0.7411249096494326\n",
      "Match probs 1.2738642556107336\n",
      "Match probs 1.2291793301714224\n",
      "Match probs 1.2913270466039672\n",
      "Match probs 1.2913270466039672\n",
      "Match probs 1.3042438684130844\n",
      "Match probs 1.2976009964806456\n",
      "Match probs 1.2976009964806456\n",
      "Match probs 1.1320465624172784\n",
      "Match probs 0.9989759921762902\n",
      "Match probs 1.0143920678987723\n",
      "Match probs 0.9359200587721559\n",
      "Match probs 0.9552156855927841\n",
      "Match probs 0.9552156855927841\n",
      "Match probs 0.8263197708268423\n",
      "Match probs 0.8009325221966593\n",
      "Match probs 1.1080784029182327\n",
      "Match probs 1.2118091278237024\n",
      "Match probs 1.0667785866602135\n",
      "Match probs 1.012960324119731\n",
      "Match probs 1.0262186308643857\n",
      "Match probs 1.0442079050374056\n",
      "Match probs 0.6450772984772886\n",
      "Match probs 1.032714433126983\n",
      "Match probs 1.240104524759159\n",
      "Match probs 1.240104524759159\n",
      "Match probs 1.2606059488660786\n",
      "Match probs 1.254736095132686\n",
      "Match probs 1.2507670908081399\n",
      "Match probs 0.8248078631403605\n",
      "Match probs 0.7795484062545596\n",
      "Match probs 1.2113774876557313\n",
      "Match probs 1.211349943631046\n",
      "Match probs 1.2113774876557313\n",
      "Match probs 0.7614467401405621\n",
      "Match probs 1.2242943094648484\n",
      "Match probs 1.1199960811177245\n",
      "Match probs 1.0445900579405247\n",
      "Match probs 1.0445900579405247\n",
      "Match probs 1.0575068797496419\n",
      "Match probs 1.2659552798869516\n",
      "Match probs 1.322233142586104\n",
      "Match probs 1.3297877373538136\n",
      "Match probs 1.3042438684130844\n",
      "Match probs 1.2630438642595812\n",
      "Match probs 1.275529045900727\n",
      "Match probs 0.8068084282243455\n",
      "Match probs 1.189737177243784\n",
      "instance 0, ep 102\n",
      "Match probs 0.8860075481754797\n",
      "Match probs 0.9236366098899707\n",
      "Match probs 1.0275576886855964\n",
      "Match probs 1.309316320776987\n",
      "Match probs 1.2626251201121443\n",
      "Match probs 1.2758834268567993\n",
      "Match probs 1.2626251201121443\n",
      "Match probs 0.9949140846525425\n",
      "Match probs 1.190939377493839\n",
      "Match probs 1.190939377493839\n",
      "Match probs 1.1637822697184377\n",
      "Match probs 1.1984939722615486\n",
      "Match probs 1.1984939722615486\n",
      "Match probs 0.8431642955398847\n",
      "Match probs 0.7811288791913299\n",
      "Match probs 0.8155487055778597\n",
      "Match probs 1.1984939722615486\n",
      "Match probs 1.3297877373538136\n",
      "Match probs 0.9612757212021664\n",
      "Match probs 0.858127832619753\n",
      "Match probs 1.0871582346638764\n",
      "Match probs 1.0871582346638764\n",
      "Match probs 0.8086274211245739\n",
      "Match probs 0.9630478617547434\n",
      "Match probs 0.9630478617547434\n",
      "Match probs 1.1729501033208194\n",
      "Match probs 1.1729501033208194\n",
      "Match probs 1.275529045900727\n",
      "Match probs 1.211324937880501\n",
      "Match probs 1.183761897693706\n",
      "Match probs 1.3042438684130844\n",
      "Match probs 1.2288378452358844\n",
      "Match probs 1.2163251195700533\n",
      "Match probs 1.1825445332662505\n",
      "Match probs 0.739988121449798\n",
      "Match probs 1.2184244557314559\n",
      "Match probs 1.0414940842018923\n",
      "Match probs 1.045511942338251\n",
      "Match probs 1.0530665371059607\n",
      "Match probs 1.0530665371059607\n",
      "Match probs 1.0742138688300737\n",
      "Match probs 0.994789987516515\n",
      "Match probs 1.0983807764563218\n",
      "Match probs 1.2023018552519478\n",
      "Match probs 0.8063413096287119\n",
      "Match probs 1.1005962874277215\n",
      "Match probs 1.0851823118774147\n",
      "Match probs 1.0534015034156405\n",
      "Match probs 0.8418747204158368\n",
      "Match probs 0.8418747204158368\n",
      "instance 0, ep 103\n",
      "Match probs 1.2243230923247443\n",
      "Match probs 0.8350947384461416\n",
      "Match probs 1.2027841854998393\n",
      "Match probs 0.789909245983758\n",
      "Match probs 1.2408389438382919\n",
      "Match probs 1.3251640366674406\n",
      "Match probs 1.3251640366674406\n",
      "Match probs 1.2735098746546611\n",
      "Match probs 1.2447828375512333\n",
      "Match probs 1.1678185563963446\n",
      "Match probs 1.2447828375512333\n",
      "Match probs 1.1205997047701337\n",
      "Match probs 0.8682055582109289\n",
      "Match probs 1.2625846800669245\n",
      "Match probs 1.322233142586104\n",
      "Match probs 1.322233142586104\n",
      "Match probs 1.2601129701782445\n",
      "Match probs 1.2601129701782445\n",
      "Match probs 1.2601129701782445\n",
      "Match probs 1.2037557724833305\n",
      "Match probs 1.247168604344442\n",
      "Match probs 0.7513721573974141\n",
      "Match probs 1.0973818261169572\n",
      "Match probs 1.1595019985248167\n",
      "Match probs 0.9174030543692612\n",
      "Match probs 0.8513831774170605\n",
      "Match probs 0.6463062624005185\n",
      "Match probs 0.8752193971367371\n",
      "Match probs 1.072697130236269\n",
      "Match probs 0.7030157652676421\n",
      "Match probs 0.8645533060571398\n",
      "Match probs 0.6750862822362872\n",
      "Match probs 1.0111929038483518\n",
      "Match probs 0.9323167161226068\n",
      "Match probs 0.6045077025205257\n",
      "Match probs 0.6226093686345231\n",
      "Match probs 1.0717146149167085\n",
      "Match probs 1.0459435825062222\n",
      "Match probs 1.26103758903405\n",
      "Match probs 0.8111068415188808\n",
      "Match probs 1.1752181763524214\n",
      "Match probs 1.0871788985955322\n",
      "Match probs 1.1280824394990732\n",
      "Match probs 1.1280824394990732\n",
      "Match probs 1.1280824394990732\n",
      "Match probs 1.3042438684130844\n",
      "Match probs 1.1729501033208194\n",
      "Match probs 1.036816575895448\n",
      "Match probs 1.0436518531797196\n",
      "Match probs 1.0436518531797196\n",
      "instance 0, ep 104\n",
      "Match probs 0.6683823991792293\n",
      "Match probs 0.4462031477430104\n",
      "Match probs 1.1980916368863908\n",
      "Match probs 0.9435590339496557\n",
      "Match probs 1.1988647619899\n",
      "Match probs 0.9901563251138302\n",
      "Match probs 1.162174137056989\n",
      "Match probs 1.0250105182313314\n",
      "Match probs 0.994789987516515\n",
      "Match probs 0.9563331507511094\n",
      "Match probs 1.1147212748506714\n",
      "Match probs 1.0393152516734714\n",
      "Match probs 0.8151082766652555\n",
      "Match probs 0.9476317860322611\n",
      "Match probs 0.9630478617547434\n",
      "Match probs 1.2005131435076144\n",
      "Match probs 1.0213479503977356\n",
      "Match probs 1.0147050784652967\n",
      "Match probs 1.1269196358185083\n",
      "Match probs 1.1489785834734294\n",
      "Match probs 1.252050504741171\n",
      "Match probs 1.2963548434336263\n",
      "Match probs 1.2963548434336263\n",
      "Match probs 1.3042438684130844\n",
      "Match probs 1.3042438684130844\n",
      "Match probs 1.3042438684130844\n",
      "Match probs 1.2005131435076144\n",
      "Match probs 1.1538219428427718\n",
      "Match probs 1.0878020658905712\n",
      "Match probs 1.1880279618664686\n",
      "Match probs 1.1628627720059965\n",
      "Match probs 1.1801389368870103\n",
      "Match probs 1.0854648309138675\n",
      "Match probs 0.6206212335886132\n",
      "Match probs 0.7591378401173527\n",
      "Match probs 0.7466251144515216\n",
      "Match probs 1.1784541958526933\n",
      "Match probs 1.1014899146978043\n",
      "Match probs 1.1147212748506714\n",
      "Match probs 1.0525735584181264\n",
      "Match probs 1.0243517145936032\n",
      "Match probs 1.2447828375512333\n",
      "Match probs 1.3247323964994693\n",
      "Match probs 1.2447828375512333\n",
      "Match probs 1.1568640887082022\n",
      "Match probs 1.154364834794837\n",
      "Match probs 1.2580662549074935\n",
      "Match probs 1.2950976352913786\n",
      "Match probs 1.2971168065374443\n",
      "Match probs 1.1658230414451793\n",
      "Took 0.16702675819396973 time for inference and 1.6659083366394043 time for training\n",
      "6.399449175671952\n"
     ]
    }
   ],
   "source": [
    "policy = greedy_policy\n",
    "name = \"greedy\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05889281507656066"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulator.match_probability_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [40 88 42 87]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 0.04671072959899902 time for inference and 0.0002827644348144531 time for training\n",
      "7.148556983964163\n"
     ]
    }
   ],
   "source": [
    "policy = random_policy\n",
    "name = \"random\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [40 88 42 87]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 0.06586885452270508 time for inference and 0.0005936622619628906 time for training\n",
      "13.314422290729675\n"
     ]
    }
   ],
   "source": [
    "policy = whittle_activity_policy\n",
    "name = \"whittle_activity\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! (0, 1) 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [80  1 98 86 89 61 50 66 18 49 17  5 40 73 23 20 24 44 32 54]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 0.2980382442474365 time for inference and 0.0035889148712158203 time for training\n",
      "3.1648230709888496\n"
     ]
    }
   ],
   "source": [
    "policy = whittle_policy\n",
    "name = \"linear_whittle\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [40 88 42 87]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 0.05080914497375488 time for inference and 5.8581366539001465 time for training\n",
      "13.572425195709457\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 4:\n",
    "    policy = q_iteration_policy\n",
    "    per_epoch_function = q_iteration_custom_epoch()\n",
    "    name = \"optimal\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],per_epoch_function=per_epoch_function,test_length=episode_len*(n_episodes%50))\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! (0, 1) 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [80  1 98 86 89 61 50 66 18 49 17  5 40 73 23 20 24 44 32 54]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 0.14577889442443848 time for inference and 0.1260237693786621 time for training\n",
      "3.1664266867321205\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 1000:\n",
    "    policy = shapley_whittle_custom_policy \n",
    "    name = \"shapley_whittle_custom\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),shapley_iterations=1000)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [82  9 66 53]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr0/home/naveenr/miniconda3/envs/food/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr0/home/naveenr/miniconda3/envs/food/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr0/home/naveenr/projects/food_rescue_rmab/rmab/mcts_policies.py:146: RuntimeWarning: divide by zero encountered in log\n",
      "  choices_weights = [(c.q() / c.n()) + c_param * np.sqrt((2 * np.log(self.n()) / c.n())) for c in self.children]\n",
      "/usr0/home/naveenr/projects/food_rescue_rmab/rmab/mcts_policies.py:146: RuntimeWarning: invalid value encountered in sqrt\n",
      "  choices_weights = [(c.q() / c.n()) + c_param * np.sqrt((2 * np.log(self.n()) / c.n())) for c in self.children]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 15.201551914215088 time for inference and 0.12212157249450684 time for training\n",
      "5.75516038777406\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 25:\n",
    "    policy = mcts_linear_policy\n",
    "    name = \"mcts_linear\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),mcts_test_iterations=400)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001668307168715904, 0.0025209872185948017, 0.0035241048773611504, 0.004228866240960799, 0.05889281507656066, 0.09711538461538462, 0.018603794437281267, 0.010603674540682414, 0.040383846461415435, 0.0014444253761226474, 0.021632041122295994, 0.019370924434215574, 0.006340636574800678, 0.0035861383326232068, 0.06408629441624365, 0.01527063804052011, 0.056361607142857144, 0.001932184125781398, 0.006313834726090994, 0.015042029199233151, 0.062461726883037354, 0.013845527351703543, 0.022972972972972974, 0.0194954128440367, 0.0017701548455840651, 0.0036838340486409155, 0.0065755873340143005, 0.002111824165009329, 0.00638640873015873, 0.03698384201077199]\n",
      "cohort [ 6 19 10  7  1  9 12 16 17  8 14  0 13  4  5  3 18 11  2 15]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "instance 0, ep 5\n",
      "instance 0, ep 6\n",
      "instance 0, ep 7\n",
      "instance 0, ep 8\n",
      "instance 0, ep 9\n",
      "instance 0, ep 10\n",
      "instance 0, ep 11\n",
      "instance 0, ep 12\n",
      "instance 0, ep 13\n",
      "instance 0, ep 14\n",
      "instance 0, ep 15\n",
      "instance 0, ep 16\n",
      "instance 0, ep 17\n",
      "instance 0, ep 18\n",
      "instance 0, ep 19\n",
      "instance 0, ep 20\n",
      "instance 0, ep 21\n",
      "instance 0, ep 22\n",
      "instance 0, ep 23\n",
      "instance 0, ep 24\n",
      "instance 0, ep 25\n",
      "instance 0, ep 26\n",
      "instance 0, ep 27\n",
      "instance 0, ep 28\n",
      "instance 0, ep 29\n",
      "instance 0, ep 30\n",
      "instance 0, ep 31\n",
      "instance 0, ep 32\n",
      "instance 0, ep 33\n",
      "instance 0, ep 34\n",
      "instance 0, ep 35\n",
      "instance 0, ep 36\n",
      "instance 0, ep 37\n",
      "instance 0, ep 38\n",
      "instance 0, ep 39\n",
      "instance 0, ep 40\n",
      "instance 0, ep 41\n",
      "instance 0, ep 42\n",
      "instance 0, ep 43\n",
      "instance 0, ep 44\n",
      "instance 0, ep 45\n",
      "instance 0, ep 46\n",
      "instance 0, ep 47\n",
      "instance 0, ep 48\n",
      "instance 0, ep 49\n",
      "instance 0, ep 50\n",
      "instance 0, ep 51\n",
      "instance 0, ep 52\n",
      "instance 0, ep 53\n",
      "instance 0, ep 54\n",
      "instance 0, ep 55\n",
      "instance 0, ep 56\n",
      "instance 0, ep 57\n",
      "instance 0, ep 58\n",
      "instance 0, ep 59\n",
      "instance 0, ep 60\n",
      "instance 0, ep 61\n",
      "instance 0, ep 62\n",
      "instance 0, ep 63\n",
      "instance 0, ep 64\n",
      "instance 0, ep 65\n",
      "instance 0, ep 66\n",
      "instance 0, ep 67\n",
      "instance 0, ep 68\n",
      "instance 0, ep 69\n",
      "instance 0, ep 70\n",
      "instance 0, ep 71\n",
      "instance 0, ep 72\n",
      "instance 0, ep 73\n",
      "instance 0, ep 74\n",
      "instance 0, ep 75\n",
      "instance 0, ep 76\n",
      "instance 0, ep 77\n",
      "instance 0, ep 78\n",
      "instance 0, ep 79\n",
      "instance 0, ep 80\n",
      "instance 0, ep 81\n",
      "instance 0, ep 82\n",
      "instance 0, ep 83\n",
      "instance 0, ep 84\n",
      "instance 0, ep 85\n",
      "instance 0, ep 86\n",
      "instance 0, ep 87\n",
      "instance 0, ep 88\n",
      "instance 0, ep 89\n",
      "instance 0, ep 90\n",
      "instance 0, ep 91\n",
      "instance 0, ep 92\n",
      "instance 0, ep 93\n",
      "instance 0, ep 94\n",
      "instance 0, ep 95\n",
      "instance 0, ep 96\n",
      "instance 0, ep 97\n",
      "instance 0, ep 98\n",
      "instance 0, ep 99\n",
      "instance 0, ep 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr0/home/naveenr/miniconda3/envs/food/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr0/home/naveenr/miniconda3/envs/food/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr0/home/naveenr/projects/food_rescue_rmab/rmab/mcts_policies.py:146: RuntimeWarning: divide by zero encountered in log\n",
      "  choices_weights = [(c.q() / c.n()) + c_param * np.sqrt((2 * np.log(self.n()) / c.n())) for c in self.children]\n",
      "/usr0/home/naveenr/projects/food_rescue_rmab/rmab/mcts_policies.py:146: RuntimeWarning: invalid value encountered in sqrt\n",
      "  choices_weights = [(c.q() / c.n()) + c_param * np.sqrt((2 * np.log(self.n()) / c.n())) for c in self.children]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance 0, ep 101\n",
      "instance 0, ep 102\n",
      "instance 0, ep 103\n",
      "instance 0, ep 104\n",
      "Took 188.50051403045654 time for inference and 2.601569175720215 time for training\n",
      "6.625318690059936\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 25:\n",
    "    policy = mcts_shapley_policy\n",
    "    name = \"mcts_shapley\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [82  9 66 53]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m policy \u001b[38;5;241m=\u001b[39m mcts_shapley_policy\n\u001b[1;32m      3\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmcts_shapley_40\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m rewards, memory, simulator \u001b[38;5;241m=\u001b[39m \u001b[43mrun_multi_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_len\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmcts_test_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_reward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name)] \u001b[38;5;241m=\u001b[39m rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_match\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name)] \u001b[38;5;241m=\u001b[39m  rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch\u001b[39m\u001b[38;5;124m'\u001b[39m] \n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/simulator.py:567\u001b[0m, in \u001b[0;36mrun_multi_seed\u001b[0;34m(seed_list, policy, parameters, should_train, per_epoch_function, avg_reward, mcts_test_iterations, mcts_depth, num_samples, shapley_iterations, test_length, max_transition_prob)\u001b[0m\n\u001b[1;32m    564\u001b[0m simulator\u001b[38;5;241m.\u001b[39mmcts_depth \u001b[38;5;241m=\u001b[39m mcts_depth\n\u001b[1;32m    565\u001b[0m simulator\u001b[38;5;241m.\u001b[39mshapley_iterations \u001b[38;5;241m=\u001b[39m shapley_iterations  \n\u001b[0;32m--> 567\u001b[0m policy_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_heterogenous_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_episodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiscount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlamb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlamb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshould_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshould_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_T\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43mget_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshould_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mper_epoch_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mper_epoch_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_cover\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_arms\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m    569\u001b[0m     match_probs \u001b[38;5;241m=\u001b[39m simulator\u001b[38;5;241m.\u001b[39mmatch_probability_list[simulator\u001b[38;5;241m.\u001b[39magent_idx]\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/simulator.py:343\u001b[0m, in \u001b[0;36mrun_heterogenous_policy\u001b[0;34m(env, n_episodes, n_epochs, discount, policy, seed, per_epoch_function, lamb, get_memory, should_train, test_T)\u001b[0m\n\u001b[1;32m    340\u001b[0m     all_active_rate[epoch,t\u001b[38;5;241m-\u001b[39m(T\u001b[38;5;241m-\u001b[39mtest_T)] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(state)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(state)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_train \u001b[38;5;129;01mor\u001b[39;00m t \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39mT\u001b[38;5;241m-\u001b[39mtest_T:\n\u001b[0;32m--> 343\u001b[0m     action,memory \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbudget\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlamb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43mper_epoch_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m     action,memory \u001b[38;5;241m=\u001b[39m random_policy(env,state,budget,lamb,memory,per_epoch_results)\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/mcts_policies.py:451\u001b[0m, in \u001b[0;36mmcts_shapley_policy\u001b[0;34m(env, state, budget, lamb, memory, per_epoch_results, group_setup, attribution_method)\u001b[0m\n\u001b[1;32m    449\u001b[0m s\u001b[38;5;241m.\u001b[39mwhittle_index \u001b[38;5;241m=\u001b[39m whittle_index(env,state,budget,lamb,memory[\u001b[38;5;241m0\u001b[39m],reward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined\u001b[39m\u001b[38;5;124m\"\u001b[39m,match_probs\u001b[38;5;241m=\u001b[39mmemory[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    450\u001b[0m root \u001b[38;5;241m=\u001b[39m MonteCarloTreeSearchNode(s,env\u001b[38;5;241m.\u001b[39mmcts_test_iterations,transitions\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mtransitions,time_limit\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mtime_limit,use_whittle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 451\u001b[0m selected_idx \u001b[38;5;241m=\u001b[39m \u001b[43mroot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbudget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m memory \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mmemory \n\u001b[1;32m    453\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(N, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint8)\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/mcts_policies.py:181\u001b[0m, in \u001b[0;36mMonteCarloTreeSearchNode.best_action\u001b[0;34m(self, budget)\u001b[0m\n\u001b[1;32m    179\u001b[0m best_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(simulation_no):\n\u001b[0;32m--> 181\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tree_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     last_action, reward \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mrollout()\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;241m>\u001b[39m best_reward:\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/mcts_policies.py:164\u001b[0m, in \u001b[0;36mMonteCarloTreeSearchNode._tree_policy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m current_node\u001b[38;5;241m.\u001b[39mexpand()\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m         current_node \u001b[38;5;241m=\u001b[39m \u001b[43mcurrent_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m current_node\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/mcts_policies.py:146\u001b[0m, in \u001b[0;36mMonteCarloTreeSearchNode.best_child\u001b[0;34m(self, c_param)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbest_child\u001b[39m(\u001b[38;5;28mself\u001b[39m, c_param\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m--> 146\u001b[0m     choices_weights \u001b[38;5;241m=\u001b[39m [(c\u001b[38;5;241m.\u001b[39mq() \u001b[38;5;241m/\u001b[39m c\u001b[38;5;241m.\u001b[39mn()) \u001b[38;5;241m+\u001b[39m c_param \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt((\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn()) \u001b[38;5;241m/\u001b[39m c\u001b[38;5;241m.\u001b[39mn())) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren]\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren[np\u001b[38;5;241m.\u001b[39margmax(choices_weights)]\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/mcts_policies.py:146\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbest_child\u001b[39m(\u001b[38;5;28mself\u001b[39m, c_param\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m--> 146\u001b[0m     choices_weights \u001b[38;5;241m=\u001b[39m [(\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m c\u001b[38;5;241m.\u001b[39mn()) \u001b[38;5;241m+\u001b[39m c_param \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt((\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn()) \u001b[38;5;241m/\u001b[39m c\u001b[38;5;241m.\u001b[39mn())) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren]\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren[np\u001b[38;5;241m.\u001b[39margmax(choices_weights)]\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/mcts_policies.py:67\u001b[0m, in \u001b[0;36mMonteCarloTreeSearchNode.q\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_children\u001b[38;5;241m.\u001b[39mvalues()))\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_of_visits\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_results\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_of_visits\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/food/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3461\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3465\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/food/lib/python3.8/site-packages/numpy/core/_methods.py:192\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    190\u001b[0m         ret \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(ret \u001b[38;5;241m/\u001b[39m rcount)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m         ret \u001b[38;5;241m=\u001b[39m \u001b[43mret\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     ret \u001b[38;5;241m=\u001b[39m ret \u001b[38;5;241m/\u001b[39m rcount\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 25:\n",
    "    policy = mcts_shapley_policy\n",
    "    name = \"mcts_shapley_40\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),mcts_test_iterations=40)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [61 54 87 93]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "instance 0, ep 5\n",
      "instance 0, ep 6\n",
      "instance 0, ep 7\n",
      "instance 0, ep 8\n",
      "instance 0, ep 9\n",
      "instance 0, ep 10\n",
      "instance 0, ep 11\n",
      "instance 0, ep 12\n",
      "instance 0, ep 13\n",
      "instance 0, ep 14\n",
      "instance 0, ep 15\n",
      "instance 0, ep 16\n",
      "instance 0, ep 17\n",
      "instance 0, ep 18\n",
      "instance 0, ep 19\n",
      "instance 0, ep 20\n",
      "instance 0, ep 21\n",
      "instance 0, ep 22\n",
      "instance 0, ep 23\n",
      "instance 0, ep 24\n",
      "instance 0, ep 25\n",
      "instance 0, ep 26\n",
      "instance 0, ep 27\n",
      "instance 0, ep 28\n",
      "instance 0, ep 29\n",
      "instance 0, ep 30\n",
      "instance 0, ep 31\n",
      "instance 0, ep 32\n",
      "instance 0, ep 33\n",
      "instance 0, ep 34\n",
      "instance 0, ep 35\n",
      "instance 0, ep 36\n",
      "instance 0, ep 37\n",
      "instance 0, ep 38\n",
      "instance 0, ep 39\n",
      "instance 0, ep 40\n",
      "instance 0, ep 41\n",
      "instance 0, ep 42\n",
      "instance 0, ep 43\n",
      "instance 0, ep 44\n",
      "instance 0, ep 45\n",
      "instance 0, ep 46\n",
      "instance 0, ep 47\n",
      "instance 0, ep 48\n",
      "instance 0, ep 49\n",
      "instance 0, ep 50\n",
      "instance 0, ep 51\n",
      "instance 0, ep 52\n",
      "instance 0, ep 53\n",
      "instance 0, ep 54\n",
      "instance 0, ep 55\n",
      "instance 0, ep 56\n",
      "instance 0, ep 57\n",
      "instance 0, ep 58\n",
      "instance 0, ep 59\n",
      "instance 0, ep 60\n",
      "instance 0, ep 61\n",
      "instance 0, ep 62\n",
      "instance 0, ep 63\n",
      "instance 0, ep 64\n",
      "instance 0, ep 65\n",
      "instance 0, ep 66\n",
      "instance 0, ep 67\n",
      "instance 0, ep 68\n",
      "instance 0, ep 69\n",
      "instance 0, ep 70\n",
      "instance 0, ep 71\n",
      "instance 0, ep 72\n",
      "instance 0, ep 73\n",
      "instance 0, ep 74\n",
      "instance 0, ep 75\n",
      "instance 0, ep 76\n",
      "instance 0, ep 77\n",
      "instance 0, ep 78\n",
      "instance 0, ep 79\n",
      "instance 0, ep 80\n",
      "instance 0, ep 81\n",
      "instance 0, ep 82\n",
      "instance 0, ep 83\n",
      "instance 0, ep 84\n",
      "instance 0, ep 85\n",
      "instance 0, ep 86\n",
      "instance 0, ep 87\n",
      "instance 0, ep 88\n",
      "instance 0, ep 89\n",
      "instance 0, ep 90\n",
      "instance 0, ep 91\n",
      "instance 0, ep 92\n",
      "instance 0, ep 93\n",
      "instance 0, ep 94\n",
      "instance 0, ep 95\n",
      "instance 0, ep 96\n",
      "instance 0, ep 97\n",
      "instance 0, ep 98\n",
      "instance 0, ep 99\n",
      "instance 0, ep 100\n",
      "instance 0, ep 101\n",
      "instance 0, ep 102\n",
      "instance 0, ep 103\n",
      "instance 0, ep 104\n",
      "Took 0.2877821922302246 time for inference and 1.0336942672729492 time for training\n",
      "6.554165272806212\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 25:\n",
    "    policy = mcts_shapley_policy\n",
    "    name = \"mcts_shapley_4\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),mcts_test_iterations=4)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [61 54 87 93]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "instance 0, ep 5\n",
      "instance 0, ep 6\n",
      "instance 0, ep 7\n",
      "instance 0, ep 8\n",
      "instance 0, ep 9\n",
      "instance 0, ep 10\n",
      "instance 0, ep 11\n",
      "instance 0, ep 12\n",
      "instance 0, ep 13\n",
      "instance 0, ep 14\n",
      "instance 0, ep 15\n",
      "instance 0, ep 16\n",
      "instance 0, ep 17\n",
      "instance 0, ep 18\n",
      "instance 0, ep 19\n",
      "instance 0, ep 20\n",
      "instance 0, ep 21\n",
      "instance 0, ep 22\n",
      "instance 0, ep 23\n",
      "instance 0, ep 24\n",
      "instance 0, ep 25\n",
      "instance 0, ep 26\n",
      "instance 0, ep 27\n",
      "instance 0, ep 28\n",
      "instance 0, ep 29\n",
      "instance 0, ep 30\n",
      "instance 0, ep 31\n",
      "instance 0, ep 32\n",
      "instance 0, ep 33\n",
      "instance 0, ep 34\n",
      "instance 0, ep 35\n",
      "instance 0, ep 36\n",
      "instance 0, ep 37\n",
      "instance 0, ep 38\n",
      "instance 0, ep 39\n",
      "instance 0, ep 40\n",
      "instance 0, ep 41\n",
      "instance 0, ep 42\n",
      "instance 0, ep 43\n",
      "instance 0, ep 44\n",
      "instance 0, ep 45\n",
      "instance 0, ep 46\n",
      "instance 0, ep 47\n",
      "instance 0, ep 48\n",
      "instance 0, ep 49\n",
      "instance 0, ep 50\n",
      "instance 0, ep 51\n",
      "instance 0, ep 52\n",
      "instance 0, ep 53\n",
      "instance 0, ep 54\n",
      "instance 0, ep 55\n",
      "instance 0, ep 56\n",
      "instance 0, ep 57\n",
      "instance 0, ep 58\n",
      "instance 0, ep 59\n",
      "instance 0, ep 60\n",
      "instance 0, ep 61\n",
      "instance 0, ep 62\n",
      "instance 0, ep 63\n",
      "instance 0, ep 64\n",
      "instance 0, ep 65\n",
      "instance 0, ep 66\n",
      "instance 0, ep 67\n",
      "instance 0, ep 68\n",
      "instance 0, ep 69\n",
      "instance 0, ep 70\n",
      "instance 0, ep 71\n",
      "instance 0, ep 72\n",
      "instance 0, ep 73\n",
      "instance 0, ep 74\n",
      "instance 0, ep 75\n",
      "instance 0, ep 76\n",
      "instance 0, ep 77\n",
      "instance 0, ep 78\n",
      "instance 0, ep 79\n",
      "instance 0, ep 80\n",
      "instance 0, ep 81\n",
      "instance 0, ep 82\n",
      "instance 0, ep 83\n",
      "instance 0, ep 84\n",
      "instance 0, ep 85\n",
      "instance 0, ep 86\n",
      "instance 0, ep 87\n",
      "instance 0, ep 88\n",
      "instance 0, ep 89\n",
      "instance 0, ep 90\n",
      "instance 0, ep 91\n",
      "instance 0, ep 92\n",
      "instance 0, ep 93\n",
      "instance 0, ep 94\n",
      "instance 0, ep 95\n",
      "instance 0, ep 96\n",
      "instance 0, ep 97\n",
      "instance 0, ep 98\n",
      "instance 0, ep 99\n",
      "instance 0, ep 100\n",
      "instance 0, ep 101\n",
      "instance 0, ep 102\n",
      "instance 0, ep 103\n",
      "instance 0, ep 104\n",
      "Took 0.18565797805786133 time for inference and 0.9187843799591064 time for training\n",
      "6.426762529732592\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 250:\n",
    "    policy = whittle_iterative_policy\n",
    "    name = \"iterative_whittle\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! (0, 1) 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [80  1 98 86 89 61 50 66 18 49 17  5 40 73 23 20 24 44 32 54]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr0/home/naveenr/projects/food_rescue_rmab/rmab/whittle_policies.py:192: RuntimeWarning: invalid value encountered in divide\n",
      "  shapley_indices = shapley_indices / num_by_shapley_index\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m policy \u001b[38;5;241m=\u001b[39m shapley_whittle_iterative_policy\n\u001b[1;32m      3\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapley_iterative_whittle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m rewards, memory, simulator \u001b[38;5;241m=\u001b[39m \u001b[43mrun_multi_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_len\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshapley_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_reward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name)] \u001b[38;5;241m=\u001b[39m rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_match\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name)] \u001b[38;5;241m=\u001b[39m  rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch\u001b[39m\u001b[38;5;124m'\u001b[39m] \n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/simulator.py:580\u001b[0m, in \u001b[0;36mrun_multi_seed\u001b[0;34m(seed_list, policy, parameters, should_train, per_epoch_function, avg_reward, mcts_test_iterations, mcts_depth, num_samples, shapley_iterations, test_length, max_transition_prob)\u001b[0m\n\u001b[1;32m    577\u001b[0m simulator\u001b[38;5;241m.\u001b[39mmcts_depth \u001b[38;5;241m=\u001b[39m mcts_depth\n\u001b[1;32m    578\u001b[0m simulator\u001b[38;5;241m.\u001b[39mshapley_iterations \u001b[38;5;241m=\u001b[39m shapley_iterations  \n\u001b[0;32m--> 580\u001b[0m policy_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_heterogenous_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_episodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiscount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlamb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlamb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshould_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshould_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_T\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43mget_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshould_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mper_epoch_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mper_epoch_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_cover\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_arms\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m    582\u001b[0m     match_probs \u001b[38;5;241m=\u001b[39m simulator\u001b[38;5;241m.\u001b[39mmatch_probability_list[simulator\u001b[38;5;241m.\u001b[39magent_idx]\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/simulator.py:343\u001b[0m, in \u001b[0;36mrun_heterogenous_policy\u001b[0;34m(env, n_episodes, n_epochs, discount, policy, seed, per_epoch_function, lamb, get_memory, should_train, test_T)\u001b[0m\n\u001b[1;32m    340\u001b[0m     all_active_rate[epoch,t\u001b[38;5;241m-\u001b[39m(T\u001b[38;5;241m-\u001b[39mtest_T)] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(state)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(state)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_train \u001b[38;5;129;01mor\u001b[39;00m t \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39mT\u001b[38;5;241m-\u001b[39mtest_T:\n\u001b[0;32m--> 343\u001b[0m     action,memory \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbudget\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlamb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43mper_epoch_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m     action,memory \u001b[38;5;241m=\u001b[39m random_policy(env,state,budget,lamb,memory,per_epoch_results)\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/whittle_policies.py:393\u001b[0m, in \u001b[0;36mshapley_whittle_iterative_policy\u001b[0;34m(env, state, budget, lamb, memory, per_epoch_results)\u001b[0m\n\u001b[1;32m    390\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(budget):\n\u001b[0;32m--> 393\u001b[0m     match_prob_now \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mshapley_index_custom_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpulled_arms\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    395\u001b[0m     state_WI \u001b[38;5;241m=\u001b[39m whittle_index(env,state,budget,lamb,memory_whittle,reward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined\u001b[39m\u001b[38;5;124m\"\u001b[39m,match_probs\u001b[38;5;241m=\u001b[39mmatch_probs,match_prob_now\u001b[38;5;241m=\u001b[39mmatch_prob_now)\n\u001b[1;32m    396\u001b[0m     state_WI[action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/whittle_policies.py:187\u001b[0m, in \u001b[0;36mshapley_index_custom_fixed\u001b[0;34m(env, state, memoizer_shapley, arms_pulled)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m combo[i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    186\u001b[0m     action[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 187\u001b[0m     shapley_indices[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcustom_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcorresponding_probabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward_parameters\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m scores[j]\n\u001b[1;32m    188\u001b[0m     num_by_shapley_index[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    189\u001b[0m     action[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/utils.py:313\u001b[0m, in \u001b[0;36mcustom_reward\u001b[0;34m(s, a, match_probabilities, custom_reward_type, reward_parameters)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m custom_reward_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    312\u001b[0m     probs \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m*\u001b[39ma\u001b[38;5;241m*\u001b[39mmatch_probabilities\n\u001b[0;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m custom_reward_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo_by_two\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    315\u001b[0m     probs \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m*\u001b[39ma\u001b[38;5;241m*\u001b[39mmatch_probabilities\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/food/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3076\u001b[0m, in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2955\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_prod_dispatcher)\n\u001b[1;32m   2956\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2957\u001b[0m          initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2958\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2959\u001b[0m \u001b[38;5;124;03m    Return the product of array elements over a given axis.\u001b[39;00m\n\u001b[1;32m   2960\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[38;5;124;03m    10\u001b[39;00m\n\u001b[1;32m   3075\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3076\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprod\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3077\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/food/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 25:\n",
    "    policy = shapley_whittle_iterative_policy\n",
    "    name = \"shapley_iterative_whittle\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),shapley_iterations=1000)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [61 54 87 93]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "instance 0, ep 5\n",
      "instance 0, ep 6\n",
      "instance 0, ep 7\n",
      "instance 0, ep 8\n",
      "instance 0, ep 9\n",
      "instance 0, ep 10\n",
      "instance 0, ep 11\n",
      "instance 0, ep 12\n",
      "instance 0, ep 13\n",
      "instance 0, ep 14\n",
      "instance 0, ep 15\n",
      "instance 0, ep 16\n",
      "instance 0, ep 17\n",
      "instance 0, ep 18\n",
      "instance 0, ep 19\n",
      "instance 0, ep 20\n",
      "instance 0, ep 21\n",
      "instance 0, ep 22\n",
      "instance 0, ep 23\n",
      "instance 0, ep 24\n",
      "instance 0, ep 25\n",
      "instance 0, ep 26\n",
      "instance 0, ep 27\n",
      "instance 0, ep 28\n",
      "instance 0, ep 29\n",
      "instance 0, ep 30\n",
      "instance 0, ep 31\n",
      "instance 0, ep 32\n",
      "instance 0, ep 33\n",
      "instance 0, ep 34\n",
      "instance 0, ep 35\n",
      "instance 0, ep 36\n",
      "instance 0, ep 37\n",
      "instance 0, ep 38\n",
      "instance 0, ep 39\n",
      "instance 0, ep 40\n",
      "instance 0, ep 41\n",
      "instance 0, ep 42\n",
      "instance 0, ep 43\n",
      "instance 0, ep 44\n",
      "instance 0, ep 45\n",
      "instance 0, ep 46\n",
      "instance 0, ep 47\n",
      "instance 0, ep 48\n",
      "instance 0, ep 49\n",
      "instance 0, ep 50\n",
      "instance 0, ep 51\n",
      "instance 0, ep 52\n",
      "instance 0, ep 53\n",
      "instance 0, ep 54\n",
      "instance 0, ep 55\n",
      "instance 0, ep 56\n",
      "instance 0, ep 57\n",
      "instance 0, ep 58\n",
      "instance 0, ep 59\n",
      "instance 0, ep 60\n",
      "instance 0, ep 61\n",
      "instance 0, ep 62\n",
      "instance 0, ep 63\n",
      "instance 0, ep 64\n",
      "instance 0, ep 65\n",
      "instance 0, ep 66\n",
      "instance 0, ep 67\n",
      "instance 0, ep 68\n",
      "instance 0, ep 69\n",
      "instance 0, ep 70\n",
      "instance 0, ep 71\n",
      "instance 0, ep 72\n",
      "instance 0, ep 73\n",
      "instance 0, ep 74\n",
      "instance 0, ep 75\n",
      "instance 0, ep 76\n",
      "instance 0, ep 77\n",
      "instance 0, ep 78\n",
      "instance 0, ep 79\n",
      "instance 0, ep 80\n",
      "instance 0, ep 81\n",
      "instance 0, ep 82\n",
      "instance 0, ep 83\n",
      "instance 0, ep 84\n",
      "instance 0, ep 85\n",
      "instance 0, ep 86\n",
      "instance 0, ep 87\n",
      "instance 0, ep 88\n",
      "instance 0, ep 89\n",
      "instance 0, ep 90\n",
      "instance 0, ep 91\n",
      "instance 0, ep 92\n",
      "instance 0, ep 93\n",
      "instance 0, ep 94\n",
      "instance 0, ep 95\n",
      "instance 0, ep 96\n",
      "instance 0, ep 97\n",
      "instance 0, ep 98\n",
      "instance 0, ep 99\n",
      "instance 0, ep 100\n",
      "instance 0, ep 101\n",
      "instance 0, ep 102\n",
      "instance 0, ep 103\n",
      "instance 0, ep 104\n",
      "Took 2.13529109954834 time for inference and 0.490398645401001 time for training\n",
      "6.426762529732592\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 50:\n",
    "    policy = shapley_whittle_iterative_policy\n",
    "    name = \"shapley_iterative_whittle_100\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),shapley_iterations=100)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [61 54 87 93]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "instance 0, ep 5\n",
      "instance 0, ep 6\n",
      "instance 0, ep 7\n",
      "instance 0, ep 8\n",
      "instance 0, ep 9\n",
      "instance 0, ep 10\n",
      "instance 0, ep 11\n",
      "instance 0, ep 12\n",
      "instance 0, ep 13\n",
      "instance 0, ep 14\n",
      "instance 0, ep 15\n",
      "instance 0, ep 16\n",
      "instance 0, ep 17\n",
      "instance 0, ep 18\n",
      "instance 0, ep 19\n",
      "instance 0, ep 20\n",
      "instance 0, ep 21\n",
      "instance 0, ep 22\n",
      "instance 0, ep 23\n",
      "instance 0, ep 24\n",
      "instance 0, ep 25\n",
      "instance 0, ep 26\n",
      "instance 0, ep 27\n",
      "instance 0, ep 28\n",
      "instance 0, ep 29\n",
      "instance 0, ep 30\n",
      "instance 0, ep 31\n",
      "instance 0, ep 32\n",
      "instance 0, ep 33\n",
      "instance 0, ep 34\n",
      "instance 0, ep 35\n",
      "instance 0, ep 36\n",
      "instance 0, ep 37\n",
      "instance 0, ep 38\n",
      "instance 0, ep 39\n",
      "instance 0, ep 40\n",
      "instance 0, ep 41\n",
      "instance 0, ep 42\n",
      "instance 0, ep 43\n",
      "instance 0, ep 44\n",
      "instance 0, ep 45\n",
      "instance 0, ep 46\n",
      "instance 0, ep 47\n",
      "instance 0, ep 48\n",
      "instance 0, ep 49\n",
      "instance 0, ep 50\n",
      "instance 0, ep 51\n",
      "instance 0, ep 52\n",
      "instance 0, ep 53\n",
      "instance 0, ep 54\n",
      "instance 0, ep 55\n",
      "instance 0, ep 56\n",
      "instance 0, ep 57\n",
      "instance 0, ep 58\n",
      "instance 0, ep 59\n",
      "instance 0, ep 60\n",
      "instance 0, ep 61\n",
      "instance 0, ep 62\n",
      "instance 0, ep 63\n",
      "instance 0, ep 64\n",
      "instance 0, ep 65\n",
      "instance 0, ep 66\n",
      "instance 0, ep 67\n",
      "instance 0, ep 68\n",
      "instance 0, ep 69\n",
      "instance 0, ep 70\n",
      "instance 0, ep 71\n",
      "instance 0, ep 72\n",
      "instance 0, ep 73\n",
      "instance 0, ep 74\n",
      "instance 0, ep 75\n",
      "instance 0, ep 76\n",
      "instance 0, ep 77\n",
      "instance 0, ep 78\n",
      "instance 0, ep 79\n",
      "instance 0, ep 80\n",
      "instance 0, ep 81\n",
      "instance 0, ep 82\n",
      "instance 0, ep 83\n",
      "instance 0, ep 84\n",
      "instance 0, ep 85\n",
      "instance 0, ep 86\n",
      "instance 0, ep 87\n",
      "instance 0, ep 88\n",
      "instance 0, ep 89\n",
      "instance 0, ep 90\n",
      "instance 0, ep 91\n",
      "instance 0, ep 92\n",
      "instance 0, ep 93\n",
      "instance 0, ep 94\n",
      "instance 0, ep 95\n",
      "instance 0, ep 96\n",
      "instance 0, ep 97\n",
      "instance 0, ep 98\n",
      "instance 0, ep 99\n",
      "instance 0, ep 100\n",
      "instance 0, ep 101\n",
      "instance 0, ep 102\n",
      "instance 0, ep 103\n",
      "instance 0, ep 104\n",
      "Took 0.06486797332763672 time for inference and 0.502739667892456 time for training\n",
      "7.422634947879637\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 50:\n",
    "    policy = shapley_whittle_iterative_policy\n",
    "    name = \"shapley_iterative_whittle_10\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len,shapley_iterations=10)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [61 54 87 93]\n",
      "instance 0, ep 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "instance 0, ep 5\n",
      "instance 0, ep 6\n",
      "instance 0, ep 7\n",
      "instance 0, ep 8\n",
      "instance 0, ep 9\n",
      "instance 0, ep 10\n",
      "instance 0, ep 11\n",
      "instance 0, ep 12\n",
      "instance 0, ep 13\n",
      "instance 0, ep 14\n",
      "instance 0, ep 15\n",
      "instance 0, ep 16\n",
      "instance 0, ep 17\n",
      "instance 0, ep 18\n",
      "instance 0, ep 19\n",
      "instance 0, ep 20\n",
      "instance 0, ep 21\n",
      "instance 0, ep 22\n",
      "instance 0, ep 23\n",
      "instance 0, ep 24\n",
      "instance 0, ep 25\n",
      "instance 0, ep 26\n",
      "instance 0, ep 27\n",
      "instance 0, ep 28\n",
      "instance 0, ep 29\n",
      "instance 0, ep 30\n",
      "instance 0, ep 31\n",
      "instance 0, ep 32\n",
      "instance 0, ep 33\n",
      "instance 0, ep 34\n",
      "instance 0, ep 35\n",
      "instance 0, ep 36\n",
      "instance 0, ep 37\n",
      "instance 0, ep 38\n",
      "instance 0, ep 39\n",
      "instance 0, ep 40\n",
      "instance 0, ep 41\n",
      "instance 0, ep 42\n",
      "instance 0, ep 43\n",
      "instance 0, ep 44\n",
      "instance 0, ep 45\n",
      "instance 0, ep 46\n",
      "instance 0, ep 47\n",
      "instance 0, ep 48\n",
      "instance 0, ep 49\n",
      "instance 0, ep 50\n",
      "instance 0, ep 51\n",
      "instance 0, ep 52\n",
      "instance 0, ep 53\n",
      "instance 0, ep 54\n",
      "instance 0, ep 55\n",
      "instance 0, ep 56\n",
      "instance 0, ep 57\n",
      "instance 0, ep 58\n",
      "instance 0, ep 59\n",
      "instance 0, ep 60\n",
      "instance 0, ep 61\n",
      "instance 0, ep 62\n",
      "instance 0, ep 63\n",
      "instance 0, ep 64\n",
      "instance 0, ep 65\n",
      "instance 0, ep 66\n",
      "instance 0, ep 67\n",
      "instance 0, ep 68\n",
      "instance 0, ep 69\n",
      "instance 0, ep 70\n",
      "instance 0, ep 71\n",
      "instance 0, ep 72\n",
      "instance 0, ep 73\n",
      "instance 0, ep 74\n",
      "instance 0, ep 75\n",
      "instance 0, ep 76\n",
      "instance 0, ep 77\n",
      "instance 0, ep 78\n",
      "instance 0, ep 79\n",
      "instance 0, ep 80\n",
      "instance 0, ep 81\n",
      "instance 0, ep 82\n",
      "instance 0, ep 83\n",
      "instance 0, ep 84\n",
      "instance 0, ep 85\n",
      "instance 0, ep 86\n",
      "instance 0, ep 87\n",
      "instance 0, ep 88\n",
      "instance 0, ep 89\n",
      "instance 0, ep 90\n",
      "instance 0, ep 91\n",
      "instance 0, ep 92\n",
      "instance 0, ep 93\n",
      "instance 0, ep 94\n",
      "instance 0, ep 95\n",
      "instance 0, ep 96\n",
      "instance 0, ep 97\n",
      "instance 0, ep 98\n",
      "instance 0, ep 99\n",
      "instance 0, ep 100\n",
      "instance 0, ep 101\n",
      "instance 0, ep 102\n",
      "instance 0, ep 103\n",
      "instance 0, ep 104\n",
      "Took 0.12944293022155762 time for inference and 0.4833834171295166 time for training\n",
      "6.617455955344084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr0/home/naveenr/projects/food_rescue_rmab/rmab/whittle_policies.py:118: RuntimeWarning: invalid value encountered in divide\n",
      "  shapley_indices /= num_by_shapley_index\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 50:\n",
    "    policy = shapley_whittle_iterative_policy\n",
    "    name = \"shapley_iterative_whittle_1\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),shapley_iterations=1)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = get_save_path(out_folder,save_name,seed,use_date=save_with_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_duplicate_results(out_folder,\"\",results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(results,open('../../results/'+save_path,'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
