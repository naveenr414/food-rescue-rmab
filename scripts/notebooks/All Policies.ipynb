{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the performance of our Whittle and Adaptive Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "import argparse \n",
    "import sys\n",
    "import secrets\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rmab.simulator import run_multi_seed, get_contextual_probabilities\n",
    "from rmab.whittle_policies import *\n",
    "from rmab.baseline_policies import *\n",
    "from rmab.mcts_policies import *\n",
    "from rmab.utils import get_save_path, delete_duplicate_results, restrict_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_jupyter = 'ipykernel' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_jupyter: \n",
    "    seed        = 43\n",
    "    n_arms      = 10\n",
    "    volunteers_per_arm = 1\n",
    "    budget      = 5\n",
    "    discount    = 0.9999\n",
    "    alpha       = 3 \n",
    "    n_episodes  = 5\n",
    "    episode_len = 5000\n",
    "    n_epochs    = 1\n",
    "    save_with_date = False \n",
    "    lamb = 0\n",
    "    prob_distro = 'uniform'\n",
    "    reward_type = \"probability\"\n",
    "    reward_parameters = {'universe_size': 20, 'arm_set_low': 0, 'arm_set_high': 1, 'recovery_rate': 0}\n",
    "    out_folder = ''\n",
    "    time_limit = 100\n",
    "    run_rate_limits = False \n",
    "else:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--n_arms',         '-N', help='num beneficiaries (arms)', type=int, default=2)\n",
    "    parser.add_argument('--volunteers_per_arm',         '-V', help='volunteers per arm', type=int, default=5)\n",
    "    parser.add_argument('--episode_len',    '-H', help='episode length', type=int, default=50)\n",
    "    parser.add_argument('--n_episodes',     '-T', help='num episodes', type=int, default=105)\n",
    "    parser.add_argument('--budget',         '-B', help='budget', type=int, default=3)\n",
    "    parser.add_argument('--n_epochs',       '-E', help='number of epochs (num_repeats)', type=int, default=1)\n",
    "    parser.add_argument('--discount',       '-d', help='discount factor', type=float, default=0.9)\n",
    "    parser.add_argument('--alpha',          '-a', help='alpha: for conf radius', type=float, default=3)\n",
    "    parser.add_argument('--lamb',          '-l', help='lambda for matching-engagement tradeoff', type=float, default=0.5)\n",
    "    parser.add_argument('--universe_size', help='For set cover, total num unvierse elems', type=int, default=10)\n",
    "    parser.add_argument('--recovery_rate', help='How fast volunteers recover', type=float, default=0.1)\n",
    "    parser.add_argument('--arm_set_low', help='Least size of arm set, for set cover', type=float, default=3)\n",
    "    parser.add_argument('--arm_set_high', help='Largest size of arm set, for set cover', type=float, default=6)\n",
    "    parser.add_argument('--reward_type',          '-r', help='Which type of custom reward', type=str, default='set_cover')\n",
    "    parser.add_argument('--seed',           '-s', help='random seed', type=int, default=42)\n",
    "    parser.add_argument('--prob_distro',           '-p', help='which prob distro [uniform,uniform_small,uniform_large,normal]', type=str, default='uniform')\n",
    "    parser.add_argument('--out_folder', help='Which folder to write results to', type=str, default='iterative')\n",
    "    parser.add_argument('--time_limit', help='Online time limit for computation', type=float, default=100)\n",
    "    parser.add_argument('--use_date', action='store_true')\n",
    "    parser.add_argument('--run_rate_limits', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    n_arms      = args.n_arms\n",
    "    volunteers_per_arm = args.volunteers_per_arm\n",
    "    budget      = args.budget\n",
    "    discount    = args.discount\n",
    "    alpha       = args.alpha \n",
    "    seed        = args.seed\n",
    "    n_episodes  = args.n_episodes\n",
    "    episode_len = args.episode_len\n",
    "    n_epochs    = args.n_epochs\n",
    "    lamb = args.lamb\n",
    "    save_with_date = args.use_date\n",
    "    prob_distro = args.prob_distro\n",
    "    out_folder = args.out_folder\n",
    "    reward_type = args.reward_type\n",
    "    run_rate_limits = args.run_rate_limits\n",
    "    recovery_rate = args.recovery_rate \n",
    "    reward_parameters = {'universe_size': args.universe_size,\n",
    "                        'arm_set_low': args.arm_set_low, \n",
    "                        'arm_set_high': args.arm_set_high, \n",
    "                        'recovery_rate': args.recovery_rate}\n",
    "    time_limit = args.time_limit \n",
    "\n",
    "save_name = secrets.token_hex(4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results['parameters'] = {'seed'      : seed,\n",
    "        'n_arms'    : n_arms,\n",
    "        'volunteers_per_arm': volunteers_per_arm, \n",
    "        'budget'    : budget,\n",
    "        'discount'  : discount, \n",
    "        'alpha'     : alpha, \n",
    "        'n_episodes': n_episodes, \n",
    "        'episode_len': episode_len, \n",
    "        'n_epochs'  : n_epochs, \n",
    "        'lamb': lamb,\n",
    "        'prob_distro': prob_distro, \n",
    "        'reward_type': reward_type, \n",
    "        'universe_size': reward_parameters['universe_size'],\n",
    "        'arm_set_low': reward_parameters['arm_set_low'], \n",
    "        'arm_set_high': reward_parameters['arm_set_high'],\n",
    "        'time_limit': time_limit, \n",
    "        'recovery_rate': reward_parameters['recovery_rate']\n",
    "        } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 43,\n",
       " 'n_arms': 10,\n",
       " 'volunteers_per_arm': 1,\n",
       " 'budget': 5,\n",
       " 'discount': 0.9999,\n",
       " 'alpha': 3,\n",
       " 'n_episodes': 5,\n",
       " 'episode_len': 5000,\n",
       " 'n_epochs': 1,\n",
       " 'lamb': 0,\n",
       " 'prob_distro': 'uniform',\n",
       " 'reward_type': 'probability',\n",
       " 'universe_size': 20,\n",
       " 'arm_set_low': 0,\n",
       " 'arm_set_high': 1,\n",
       " 'time_limit': 100,\n",
       " 'recovery_rate': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['parameters']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list = [seed]\n",
    "restrict_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy\n",
      "cohort [87 53 47  7 61 18 10 46 21 96]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 6.445762395858765 time for inference and 0.02349400520324707 time for training\n",
      "3721.2745392691895\n"
     ]
    }
   ],
   "source": [
    "policy = greedy_policy\n",
    "name = \"greedy\"\n",
    "print(name)\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "results['{}_burned_out_rate'.format(name)] =  rewards['burned_out_rate']\n",
    "results['ratio'] = simulator.ratio \n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random\n",
      "cohort [39 29 82 85 87 17 45 10 41 54]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 8.945305109024048 time for inference and 0.0004248619079589844 time for training\n",
      "0.04433299800100805\n"
     ]
    }
   ],
   "source": [
    "policy = random_policy\n",
    "name = \"random\"\n",
    "print(name)\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "results['{}_burned_out_rate'.format(name)] =  rewards['burned_out_rate']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_arms * volunteers_per_arm <= 4 and 'multi_state' not in prob_distro and 'two_timescale' not in prob_distro:\n",
    "    policy = q_iteration_policy\n",
    "    per_epoch_function = q_iteration_custom_epoch()\n",
    "    name = \"optimal\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],per_epoch_function=per_epoch_function,test_length=episode_len*(n_episodes%50))\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    results['{}_burned_out_rate'.format(name)] =  rewards['burned_out_rate']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whittle_activity\n",
      "cohort [39 29 82 85 87 17 45 10 41 54]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 9.227784395217896 time for inference and 0.0006270408630371094 time for training\n",
      "0.0027551598135849577\n"
     ]
    }
   ],
   "source": [
    "policy = whittle_activity_policy\n",
    "name = \"whittle_activity\"\n",
    "print(name)\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "results['{}_burned_out_rate'.format(name)] =  rewards['burned_out_rate']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_whittle\n",
      "cohort [39 29 82 85 87 17 45 10 41 54]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 9.158974170684814 time for inference and 0.002565145492553711 time for training\n",
      "0.0883771102599891\n"
     ]
    }
   ],
   "source": [
    "policy = whittle_policy\n",
    "name = \"linear_whittle\"\n",
    "print(name)\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "results['{}_burned_out_rate'.format(name)] =  rewards['burned_out_rate']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'context' in reward_type and n_arms * volunteers_per_arm <= 250:\n",
    "    policy = fast_contextual_whittle_policy\n",
    "    name = \"fast_contextual_whittle\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_arms * volunteers_per_arm <= 250 and 'context' in reward_type  and 'two_timescale' not in prob_distro:\n",
    "    policy = contextual_whittle_policy\n",
    "    name = \"contextual_whittle\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapley_whittle_custom\n",
      "cohort [39 29 82 85 87 17 45 10 41 54]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 9.155778169631958 time for inference and 3.6136789321899414 time for training\n",
      "0.0883771102599891\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 1000:\n",
    "    policy = shapley_whittle_custom_policy \n",
    "    name = \"shapley_whittle_custom\"\n",
    "    print(name)\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),shapley_iterations=1000)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    results['{}_burned_out_rate'.format(name)] =  rewards['burned_out_rate']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_arms * volunteers_per_arm <= 250 and 'context' in reward_type and episode_len<=10000:\n",
    "    policy = fast_contextual_shapley_policy\n",
    "    name = \"fast_contextual_shapley\"\n",
    "    print(name)\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_arms * volunteers_per_arm <= 250 and 'context' in reward_type and 'two_timescale' not in prob_distro:\n",
    "    policy = contextual_shapley_policy\n",
    "    name = \"contextual_shapley\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [39 29 82 85 87 17 45 10 41 54]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 47.20218276977539 time for inference and 0.008659601211547852 time for training\n",
      "0.0883771102599891\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 250 and episode_len<=10000:\n",
    "    policy = whittle_iterative_policy\n",
    "    name = \"iterative_whittle\"\n",
    "    print(name)\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    results['{}_burned_out_rate'.format(name)] =  rewards['burned_out_rate']\n",
    "\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_arms * volunteers_per_arm <= 250 and 'context' in reward_type and episode_len <= 10000:\n",
    "    policy = non_contextual_whittle_iterative_policy\n",
    "    name = \"non_contextual_iterative_whittle\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    results['{}_burned_out_rate'.format(name)] =  rewards['burned_out_rate']\n",
    "\n",
    "    print(np.mean(rewards['reward']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohort [39 29 82 85 87 17 45 10 41 54]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 57.01513719558716 time for inference and 3.6723501682281494 time for training\n",
      "0.0883771102599891\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 25 and reward_type != 'probability_context' and episode_len<=10000:\n",
    "    policy = shapley_whittle_iterative_policy\n",
    "    name = \"shapley_iterative_whittle\"\n",
    "    print(name)\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),shapley_iterations=1000)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    results['{}_burned_out_rate'.format(name)] =  rewards['burned_out_rate']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_arms * volunteers_per_arm <= 250 and 'context' in reward_type and episode_len <= 10000:\n",
    "    policy = non_contextual_shapley_whittle_iterative_policy\n",
    "    name = \"non_contextual_shapley_iterative_whittle\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    results['{}_burned_out_rate'.format(name)] =  rewards['burned_out_rate']\n",
    "\n",
    "    print(np.mean(rewards['reward']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_arms * volunteers_per_arm <= 25 and 'two_timescale' not in prob_distro and 'multi_state' not in prob_distro:\n",
    "    policy = mcts_linear_policy\n",
    "    name = \"mcts_linear\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),mcts_test_iterations=400)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    results['{}_burned_out_rate'.format(name)] =  rewards['burned_out_rate']\n",
    "    print(np.mean(rewards['reward']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_arms * volunteers_per_arm <= 250 and 'two_timescale' not in prob_distro and 'context' in reward_type:\n",
    "    policy = non_contextual_mcts_linear_policy\n",
    "    name = \"non_contextual_mcts_linear\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    results['{}_burned_out_rate'.format(name)] =  rewards['burned_out_rate']\n",
    "\n",
    "    print(np.mean(rewards['reward']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_arms * volunteers_per_arm <= 25 and 'two_timescale' not in prob_distro and 'multi_state' not in prob_distro:\n",
    "    policy = mcts_shapley_policy\n",
    "    name = \"mcts_shapley\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),mcts_test_iterations=400)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    results['{}_burned_out_rate'.format(name)] =  rewards['burned_out_rate']\n",
    "\n",
    "    print(np.mean(rewards['reward']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_arms * volunteers_per_arm <= 250 and 'two_timescale' not in prob_distro and 'context' in reward_type:\n",
    "    policy = non_contextual_mcts_shapley_policy\n",
    "    name = \"non_contextual_mcts_shapley\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50))\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    results['{}_burned_out_rate'.format(name)] =  rewards['burned_out_rate']\n",
    "\n",
    "    print(np.mean(rewards['reward']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_arms * volunteers_per_arm <= 25 and run_rate_limits:\n",
    "    policy = mcts_shapley_policy\n",
    "    name = \"mcts_shapley_40\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),mcts_test_iterations=40)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_arms * volunteers_per_arm <= 25 and run_rate_limits:\n",
    "    policy = mcts_shapley_policy\n",
    "    name = \"mcts_shapley_4\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),mcts_test_iterations=4)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_arms * volunteers_per_arm <= 50 and run_rate_limits:\n",
    "    policy = shapley_whittle_iterative_policy\n",
    "    name = \"shapley_iterative_whittle_100\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),shapley_iterations=100)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_arms * volunteers_per_arm <= 50 and run_rate_limits:\n",
    "    policy = shapley_whittle_iterative_policy\n",
    "    name = \"shapley_iterative_whittle_10\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len,shapley_iterations=10)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_arms * volunteers_per_arm <= 50 and run_rate_limits:\n",
    "    policy = shapley_whittle_iterative_policy\n",
    "    name = \"shapley_iterative_whittle_1\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,results['parameters'],test_length=episode_len*(n_episodes%50),shapley_iterations=1)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = get_save_path(out_folder,save_name,seed,use_date=save_with_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_duplicate_results(out_folder,\"\",results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(results,open('../../results/'+save_path,'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
