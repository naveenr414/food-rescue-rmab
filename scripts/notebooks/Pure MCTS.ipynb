{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi Synthetic Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the performance of various algorithms to solve the joint matching + activity task, when the number of volunteers is large and structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "import argparse \n",
    "import sys\n",
    "import secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr0/home/naveenr/miniconda3/envs/food/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from rmab.simulator import RMABSimulator\n",
    "from rmab.omniscient_policies import *\n",
    "from rmab.fr_dynamics import get_all_transitions\n",
    "from rmab.mcts_policies import *\n",
    "from rmab.utils import get_save_path, delete_duplicate_results, create_prob_distro\n",
    "import resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_per_process_memory_fraction(0.5)\n",
    "torch.set_num_threads(1)\n",
    "resource.setrlimit(resource.RLIMIT_AS, (30 * 1024 * 1024 * 1024, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_jupyter = 'ipykernel' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_jupyter: \n",
    "    seed        = 43\n",
    "    n_arms      = 2\n",
    "    volunteers_per_arm = 2\n",
    "    budget      = 2\n",
    "    discount    = 0.9\n",
    "    alpha       = 3 \n",
    "    n_episodes  = 1\n",
    "    episode_len = 20 \n",
    "    n_epochs    = 3\n",
    "    save_with_date = False \n",
    "    TIME_PER_RUN = 0.01 * 1000\n",
    "    lamb = 0.5\n",
    "    prob_distro = 'uniform'\n",
    "    policy_lr=5e-3\n",
    "    value_lr=1e-4\n",
    "    train_iterations = 30\n",
    "    test_iterations = 30\n",
    "    out_folder = 'mcts_exploration/mcts_exploration'\n",
    "else:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--n_arms',         '-N', help='num beneficiaries (arms)', type=int, default=2)\n",
    "    parser.add_argument('--volunteers_per_arm',         '-V', help='volunteers per arm', type=int, default=5)\n",
    "    parser.add_argument('--episode_len',    '-H', help='episode length', type=int, default=20)\n",
    "    parser.add_argument('--n_episodes',     '-T', help='num episodes', type=int, default=1)\n",
    "    parser.add_argument('--budget',         '-B', help='budget', type=int, default=3)\n",
    "    parser.add_argument('--n_epochs',       '-E', help='number of epochs (num_repeats)', type=int, default=3)\n",
    "    parser.add_argument('--discount',       '-d', help='discount factor', type=float, default=0.9)\n",
    "    parser.add_argument('--alpha',          '-a', help='alpha: for conf radius', type=float, default=3)\n",
    "    parser.add_argument('--lamb',          '-l', help='lambda for matching-engagement tradeoff', type=float, default=0.5)\n",
    "    parser.add_argument('--seed',           '-s', help='random seed', type=int, default=42)\n",
    "    parser.add_argument('--prob_distro',           '-p', help='which prob distro [uniform,uniform_small,uniform_large,normal]', type=str, default='uniform')\n",
    "    parser.add_argument('--time_per_run',      '-t', help='time per MCTS run', type=float, default=.01*1000)\n",
    "    parser.add_argument('--policy_lr', help='Learning Rate Policy', type=float, default=5e-3)\n",
    "    parser.add_argument('--value_lr', help='Learning Rate Value', type=float, default=1e-4)\n",
    "    parser.add_argument('--train_iterations', help='Number of MCTS train iterations', type=int, default=30)\n",
    "    parser.add_argument('--test_iterations', help='Number of MCTS test iterations', type=int, default=30)\n",
    "    parser.add_argument('--out_folder', help='Which folder to write results to', type=str, default='mcts_exploration/mcts_exploration')\n",
    "\n",
    "    parser.add_argument('--use_date', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    n_arms      = args.n_arms\n",
    "    volunteers_per_arm = args.volunteers_per_arm\n",
    "    budget      = args.budget\n",
    "    discount    = args.discount\n",
    "    alpha       = args.alpha \n",
    "    seed        = args.seed\n",
    "    n_episodes  = args.n_episodes\n",
    "    episode_len = args.episode_len\n",
    "    n_epochs    = args.n_epochs\n",
    "    lamb = args.lamb\n",
    "    save_with_date = args.use_date\n",
    "    TIME_PER_RUN = args.time_per_run\n",
    "    prob_distro = args.prob_distro\n",
    "    policy_lr = args.policy_lr \n",
    "    value_lr = args.value_lr \n",
    "    out_folder = args.out_folder\n",
    "    train_iterations = args.train_iterations \n",
    "    test_iterations = args.test_iterations \n",
    "\n",
    "save_name = secrets.token_hex(4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 2\n",
    "n_actions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_population_size = 100 # number of random arms to generate\n",
    "all_transitions = get_all_transitions(all_population_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if prob_distro == 'uniform':\n",
    "        match_probabilities = [np.random.random() for i in range(all_population_size * volunteers_per_arm)] \n",
    "    elif prob_distro == 'uniform_small':\n",
    "        match_probabilities = [np.random.random()/4 for i in range(all_population_size * volunteers_per_arm)] \n",
    "    elif prob_distro == 'uniform_large':\n",
    "        match_probabilities = [np.random.random()/4+0.75 for i in range(all_population_size * volunteers_per_arm)] \n",
    "    elif prob_distro == 'normal':\n",
    "        match_probabilities = [np.clip(np.random.normal(0.25, 0.1),0,1) for i in range(all_population_size * volunteers_per_arm)] \n",
    "\n",
    "    all_features = np.arange(all_population_size)\n",
    "    match_probabilities = create_prob_distro(prob_distro,all_population_size*volunteers_per_arm)\n",
    "    simulator = RMABSimulator(all_population_size, all_features, all_transitions,\n",
    "                n_arms, volunteers_per_arm, episode_len, n_epochs, n_episodes, budget, discount,number_states=n_states, reward_style='match',match_probability_list=match_probabilities,TIME_PER_RUN=TIME_PER_RUN)\n",
    "\n",
    "    return simulator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_seed(seed_list,policy,is_mcts=False,per_epoch_function=None,train_iterations=0,test_iterations=0,test_length=20):\n",
    "    memories = []\n",
    "    scores = {\n",
    "        'reward': [],\n",
    "        'time': [], \n",
    "        'match': [], \n",
    "        'active_rate': [],\n",
    "    }\n",
    "\n",
    "    for seed in seed_list:\n",
    "        simulator = create_environment(seed)\n",
    "        if is_mcts:\n",
    "            simulator.mcts_train_iterations = train_iterations\n",
    "            simulator.mcts_test_iterations = test_iterations\n",
    "            simulator.policy_lr = policy_lr\n",
    "            simulator.value_lr = value_lr\n",
    "\n",
    "        if is_mcts:\n",
    "            match, active_rate, memory = run_heterogenous_policy(simulator, n_episodes, n_epochs, discount,policy,seed,lamb=lamb,should_train=True,test_T=test_length,get_memory=True,per_epoch_function=per_epoch_function)\n",
    "        else:\n",
    "            match, active_rate = run_heterogenous_policy(simulator, n_episodes, n_epochs, discount,policy,seed,lamb=lamb,should_train=True,test_T=test_length,per_epoch_function=per_epoch_function)\n",
    "        time_whittle = simulator.time_taken\n",
    "        discounted_reward = get_discounted_reward(match,active_rate,discount,lamb)\n",
    "        scores['reward'].append(discounted_reward)\n",
    "        scores['time'].append(time_whittle)\n",
    "        scores['match'].append(np.mean(match))\n",
    "        scores['active_rate'].append(np.mean(active_rate))\n",
    "        if is_mcts:\n",
    "            memories.append(memory)\n",
    "\n",
    "    return scores, memories, simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results['parameters'] = {'seed'      : seed,\n",
    "        'n_arms'    : n_arms,\n",
    "        'volunteers_per_arm': volunteers_per_arm, \n",
    "        'budget'    : budget,\n",
    "        'discount'  : discount, \n",
    "        'alpha'     : alpha, \n",
    "        'n_episodes': n_episodes, \n",
    "        'episode_len': episode_len, \n",
    "        'n_epochs'  : n_epochs, \n",
    "        'lamb': lamb,\n",
    "        'time_per_run': TIME_PER_RUN, \n",
    "        'prob_distro': prob_distro, \n",
    "        'policy_lr': policy_lr, \n",
    "        'value_lr': value_lr} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list = [seed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [96 93 95 90  2 91 76 19 97 36]\n",
      "cohort [49 95  0 85  3 11 51  8 81 18]\n",
      "cohort [35 37 52 94 97 91 33 55 17 54]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m policy \u001b[38;5;241m=\u001b[39m whittle_policy\n\u001b[1;32m      2\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_whittle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m rewards, memory, simulator \u001b[38;5;241m=\u001b[39m \u001b[43mrun_multi_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_reward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name)] \u001b[38;5;241m=\u001b[39m rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_match\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name)] \u001b[38;5;241m=\u001b[39m  rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch\u001b[39m\u001b[38;5;124m'\u001b[39m] \n",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m, in \u001b[0;36mrun_multi_seed\u001b[0;34m(seed_list, policy, is_mcts, per_epoch_function, train_iterations, test_iterations, test_length)\u001b[0m\n\u001b[1;32m     19\u001b[0m     match, active_rate, memory \u001b[38;5;241m=\u001b[39m run_heterogenous_policy(simulator, n_episodes, n_epochs, discount,policy,seed,lamb\u001b[38;5;241m=\u001b[39mlamb,should_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,test_T\u001b[38;5;241m=\u001b[39mtest_length,get_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,per_epoch_function\u001b[38;5;241m=\u001b[39mper_epoch_function)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     match, active_rate \u001b[38;5;241m=\u001b[39m \u001b[43mrun_heterogenous_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlamb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlamb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshould_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mtest_T\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43mper_epoch_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mper_epoch_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m time_whittle \u001b[38;5;241m=\u001b[39m simulator\u001b[38;5;241m.\u001b[39mtime_taken\n\u001b[1;32m     23\u001b[0m discounted_reward \u001b[38;5;241m=\u001b[39m get_discounted_reward(match,active_rate,discount,lamb)\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/omniscient_policies.py:1003\u001b[0m, in \u001b[0;36mrun_heterogenous_policy\u001b[0;34m(env, n_episodes, n_epochs, discount, policy, seed, per_epoch_function, lamb, get_memory, should_train, test_T)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1001\u001b[0m     all_active_rate[epoch,t] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(state)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(state)\n\u001b[0;32m-> 1003\u001b[0m action,memory \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbudget\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlamb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43mper_epoch_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1004\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m T: env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/omniscient_policies.py:290\u001b[0m, in \u001b[0;36mwhittle_policy\u001b[0;34m(env, state, budget, lamb, memory, per_epoch_results)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     memoizer \u001b[38;5;241m=\u001b[39m memory \n\u001b[0;32m--> 290\u001b[0m state_WI \u001b[38;5;241m=\u001b[39m \u001b[43mwhittle_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbudget\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlamb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmemoizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m sorted_WI \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(state_WI)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    293\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(N, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint8)\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/omniscient_policies.py:62\u001b[0m, in \u001b[0;36mwhittle_index\u001b[0;34m(env, state, budget, lamb, memoizer, reward_function, shapley_values, contextual, match_probs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     state_WI[i] \u001b[38;5;241m=\u001b[39m check_set_val\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     state_WI[i] \u001b[38;5;241m=\u001b[39m \u001b[43marm_compute_whittle\u001b[49m\u001b[43m(\u001b[49m\u001b[43marm_transitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_chosen_subsidy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreward_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlamb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlamb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmatch_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatch_probability_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_arms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reward_function \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     64\u001b[0m         memoizer\u001b[38;5;241m.\u001b[39madd_set(arm_transitions, state[i], state_WI[i])\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/compute_whittle.py:221\u001b[0m, in \u001b[0;36marm_compute_whittle\u001b[0;34m(transitions, state, discount, subsidy_break, eps, reward_function, lamb, match_prob, match_probability_list, get_v, num_arms)\u001b[0m\n\u001b[1;32m    218\u001b[0m     action, value, v_val \u001b[38;5;241m=\u001b[39m arm_value_v_iteration(transitions, state, predicted_subsidy, discount,reward_function\u001b[38;5;241m=\u001b[39mreward_function,lamb\u001b[38;5;241m=\u001b[39mlamb,\n\u001b[1;32m    219\u001b[0m                 match_prob\u001b[38;5;241m=\u001b[39mmatch_prob)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43marm_value_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_subsidy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreward_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlamb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlamb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmatch_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatch_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_arms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_arms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# optimal action is passive: subsidy is too high\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     ub \u001b[38;5;241m=\u001b[39m predicted_subsidy\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/compute_whittle.py:179\u001b[0m, in \u001b[0;36marm_value_iteration\u001b[0;34m(transitions, state, predicted_subsidy, discount, threshold, reward_function, lamb, match_prob, num_arms)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marm_value_iteration\u001b[39m(transitions, state, predicted_subsidy, discount, threshold\u001b[38;5;241m=\u001b[39mvalue_iteration_threshold,reward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity\u001b[39m\u001b[38;5;124m'\u001b[39m,lamb\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    173\u001b[0m                         match_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,num_arms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    174\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" value iteration for a single arm at a time\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m    value iteration for the MDP defined by transitions with lambda-adjusted reward function\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m    return action corresponding to pi^*(s_I)\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mget_q_vals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpredicted_subsidy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdiscount\u001b[49m\u001b[43m,\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreward_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlamb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlamb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmatch_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatch_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_arms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_arms\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/compute_whittle.py:62\u001b[0m, in \u001b[0;36mget_q_vals\u001b[0;34m(transitions, state, predicted_subsidy, discount, threshold, reward_function, lamb, match_prob, get_v, num_arms)\u001b[0m\n\u001b[1;32m     59\u001b[0m         Q_func[s, a] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m transitions[s, a]) \u001b[38;5;241m*\u001b[39m (r(s, a) \u001b[38;5;241m+\u001b[39m discount \u001b[38;5;241m*\u001b[39m value_func[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# # transitioning to state = 1\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m         Q_func[s, a] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m transitions[s, a] \u001b[38;5;241m*\u001b[39m (\u001b[43mr\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m discount \u001b[38;5;241m*\u001b[39m value_func[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     64\u001b[0m     value_func[s] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(Q_func[s, :])\n\u001b[1;32m     66\u001b[0m difference \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(orig_value_func \u001b[38;5;241m-\u001b[39m value_func)\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/compute_whittle.py:39\u001b[0m, in \u001b[0;36mget_q_vals.<locals>.combined_reward\u001b[0;34m(s, a)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombined_reward\u001b[39m(s,a):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\u001b[38;5;241m*\u001b[39ma\u001b[38;5;241m*\u001b[39mmatch_prob\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mlamb) \u001b[38;5;241m+\u001b[39m \u001b[43mlamb\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mnum_arms\u001b[49m \u001b[38;5;241m-\u001b[39m a\u001b[38;5;241m*\u001b[39mpredicted_subsidy\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "policy = whittle_policy\n",
    "name = \"linear_whittle\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,test_length=20)\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [73 66]\n",
      "cohort [70 38]\n",
      "cohort [19  6]\n",
      "Took 0.0060045719146728516 time for inference and 9.039716243743896 time for training\n",
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [26 66]\n",
      "cohort [96 58]\n",
      "cohort [29 41]\n",
      "Took 0.005823850631713867 time for inference and 8.660557270050049 time for training\n",
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [44  8]\n",
      "cohort [98  1]\n",
      "cohort [32 64]\n",
      "Took 0.005775928497314453 time for inference and 9.150004863739014 time for training\n",
      "1.8096555925777384\n"
     ]
    }
   ],
   "source": [
    "if n_arms * volunteers_per_arm <= 4:\n",
    "    policy = q_iteration_policy\n",
    "    per_epoch_function = q_iteration_epoch\n",
    "    name = \"optimal\"\n",
    "\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,per_epoch_function=per_epoch_function,test_length=20,test_iterations=100)\n",
    "    results['{}_reward'.format(name)] = rewards['reward']\n",
    "    results['{}_match'.format(name)] =  rewards['match'] \n",
    "    results['{}_active'.format(name)] = rewards['active_rate']\n",
    "    results['{}_time'.format(name)] =  rewards['time']\n",
    "    print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [96 93 95 90  2 91 76 19 97 36]\n",
      "cohort [49 95  0 85  3 11 51  8 81 18]\n",
      "cohort [35 37 52 94 97 91 33 55 17 54]\n",
      "Took 49.74061298370361 time for inference and 2.6248719692230225 time for training\n",
      "1.7241358480844118\n"
     ]
    }
   ],
   "source": [
    "policy = mcts_policy\n",
    "name = \"mcts\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,test_length=20,is_mcts=True,test_iterations=100)\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [26 66]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 9.26994514465332 time for inference and 0.09946060180664062 time for training\n",
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [26 66]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 17.843254804611206 time for inference and 0.18002963066101074 time for training\n",
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [26 66]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 26.63034677505493 time for inference and 0.26439571380615234 time for training\n",
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [26 66]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 34.88342261314392 time for inference and 0.3500339984893799 time for training\n",
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [26 66]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 43.534244775772095 time for inference and 0.43130064010620117 time for training\n",
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [26 66]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 84.42963099479675 time for inference and 0.831467866897583 time for training\n",
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [26 66]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 164.9167845249176 time for inference and 1.8572165966033936 time for training\n"
     ]
    }
   ],
   "source": [
    "reward_by_iterations = {}\n",
    "reward_by_iterations[100] = rewards['reward']\n",
    "policy = mcts_policy\n",
    "\n",
    "for test_iterations in [50,150,250]:\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,test_length=20,is_mcts=True,test_iterations=test_iterations)\n",
    "    reward_by_iterations[test_iterations] = rewards['reward']\n",
    "results[\"mcts_test_iterations\"] = reward_by_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [26 66]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 119.4406042098999 time for inference and 1.4624977111816406 time for training\n",
      "1.5275442994961703\n"
     ]
    }
   ],
   "source": [
    "policy = mcts_whittle_policy\n",
    "name = \"mcts_whittle\"\n",
    "\n",
    "rewards, memory, simulator = run_multi_seed(seed_list,policy,test_length=20,is_mcts=True,test_iterations=100)\n",
    "results['{}_reward'.format(name)] = rewards['reward']\n",
    "results['{}_match'.format(name)] =  rewards['match'] \n",
    "results['{}_active'.format(name)] = rewards['active_rate']\n",
    "results['{}_time'.format(name)] =  rewards['time']\n",
    "print(np.mean(rewards['reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [26 66]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 13.911214351654053 time for inference and 0.14429521560668945 time for training\n",
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [26 66]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 26.663506984710693 time for inference and 0.2675325870513916 time for training\n",
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [26 66]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 62.609358072280884 time for inference and 0.6261565685272217 time for training\n",
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [26 66]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 120.16832447052002 time for inference and 1.1868908405303955 time for training\n",
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [26 66]\n",
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "Took 226.897794008255 time for inference and 2.2430570125579834 time for training\n"
     ]
    }
   ],
   "source": [
    "reward_by_iterations_whittle = {}\n",
    "reward_by_iterations_whittle[100] = rewards['reward']\n",
    "policy = mcts_whittle_policy\n",
    "name = \"mcts_whittle\"\n",
    "\n",
    "for test_iterations in [50,150,250]:\n",
    "    rewards, memory, simulator = run_multi_seed(seed_list,policy,test_length=20,is_mcts=True,test_iterations=test_iterations)\n",
    "    reward_by_iterations_whittle[test_iterations] = rewards['reward']\n",
    "results[\"mcts_whittle_test_iterations\"] = reward_by_iterations_whittle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = get_save_path(out_folder,save_name,seed,use_date=save_with_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_duplicate_results(out_folder,\"\",results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(results,open('../../results/'+save_path,'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
