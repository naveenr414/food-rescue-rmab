{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the performance of various algorithms to solve the joint matching + activity task, when the number of volunteers is large and structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "import argparse \n",
    "import sys\n",
    "import secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rmab.simulator import RMABSimulator\n",
    "from rmab.omniscient_policies import *\n",
    "from rmab.fr_dynamics import get_all_transitions, get_match_probs, get_dict_match_probs\n",
    "from rmab.mcts_policies import full_mcts_policy\n",
    "from rmab.utils import get_save_path, delete_duplicate_results\n",
    "import resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_per_process_memory_fraction(0.5)\n",
    "torch.set_num_threads(1)\n",
    "resource.setrlimit(resource.RLIMIT_AS, (20 * 1024 * 1024 * 1024, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/food/lib/python3.8/site-packages/IPython/core/async_helpers.py:129\u001b[0m, in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     coro\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    130\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    131\u001b[0m     \u001b[39mreturn\u001b[39;00m exc\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/miniconda3/envs/food/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3214\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[0m\n\u001b[1;32m   3212\u001b[0m \u001b[39m# Store raw and processed history\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[39mif\u001b[39;00m store_history:\n\u001b[0;32m-> 3214\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistory_manager\u001b[39m.\u001b[39;49mstore_inputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecution_count, cell, raw_cell)\n\u001b[1;32m   3215\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m silent:\n\u001b[1;32m   3216\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mlog(cell, raw_cell)\n",
      "File \u001b[0;32m~/miniconda3/envs/food/lib/python3.8/site-packages/IPython/core/history.py:773\u001b[0m, in \u001b[0;36mHistoryManager.store_inputs\u001b[0;34m(self, line_num, source, source_raw)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_hist_parsed\u001b[39m.\u001b[39mappend(source)\n\u001b[1;32m    771\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_hist_raw\u001b[39m.\u001b[39mappend(source_raw)\n\u001b[0;32m--> 773\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdb_input_cache_lock:\n\u001b[1;32m    774\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdb_input_cache\u001b[39m.\u001b[39mappend((line_num, source, source_raw))\n\u001b[1;32m    775\u001b[0m     \u001b[39m# Trigger to flush cache and write to DB.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "is_jupyter = 'ipykernel' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_jupyter: \n",
    "    seed        = 42\n",
    "    n_arms      = 100 # TODO: Change this to 100\n",
    "    volunteers_per_arm = 100\n",
    "    budget      = 3\n",
    "    discount    = 0.9\n",
    "    alpha       = 3 \n",
    "    n_episodes  = 125 \n",
    "    episode_len = 20 \n",
    "    n_epochs    = 1\n",
    "    save_with_date = False \n",
    "    TIME_PER_RUN = 0.01 * 1000\n",
    "    lamb = 0.5\n",
    "else:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--n_arms',         '-N', help='num beneficiaries (arms)', type=int, default=100)\n",
    "    parser.add_argument('--volunteers_per_arm',         '-V', help='volunteers per arm', type=int, default=100)\n",
    "    parser.add_argument('--episode_len',    '-H', help='episode length', type=int, default=20)\n",
    "    parser.add_argument('--n_episodes',     '-T', help='num episodes', type=int, default=30)\n",
    "    parser.add_argument('--budget',         '-B', help='budget', type=int, default=3)\n",
    "    parser.add_argument('--n_epochs',       '-E', help='number of epochs (num_repeats)', type=int, default=1)\n",
    "    parser.add_argument('--discount',       '-d', help='discount factor', type=float, default=0.9)\n",
    "    parser.add_argument('--alpha',          '-a', help='alpha: for conf radius', type=float, default=3)\n",
    "    parser.add_argument('--lamb',          '-l', help='lambda for matching-engagement tradeoff', type=float, default=1)\n",
    "    parser.add_argument('--seed',           '-s', help='random seed', type=int, default=42)\n",
    "    parser.add_argument('--time_per_run',      '-t', help='time per MCTS run', type=float, default=.01*1000)\n",
    "    parser.add_argument('--use_date', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    n_arms      = args.n_arms\n",
    "    volunteers_per_arm = args.volunteers_per_arm\n",
    "    budget      = args.budget\n",
    "    discount    = args.discount\n",
    "    alpha       = args.alpha \n",
    "    seed        = args.seed\n",
    "    n_episodes  = args.n_episodes\n",
    "    episode_len = args.episode_len\n",
    "    n_epochs    = args.n_epochs\n",
    "    lamb = args.lamb /(volunteers_per_arm*n_arms)\n",
    "    save_with_date = args.use_date\n",
    "    TIME_PER_RUN = args.time_per_run\n",
    "\n",
    "save_name = secrets.token_hex(4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 2\n",
    "n_actions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_population_size = 100 # number of random arms to generate\n",
    "all_transitions = get_all_transitions(all_population_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = np.arange(all_population_size)\n",
    "\n",
    "match_probabilities = get_match_probs([i//volunteers_per_arm+1 for i in range(all_population_size * volunteers_per_arm)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acting should always be good! 0.000 < 0.044\n",
      "acting should always be good! 0.000 < 0.162\n",
      "acting should always be good! 0.108 < 0.183\n",
      "good start state should always be good! 0.380 < 0.508\n",
      "good start state should always be good! 0.506 < 0.760\n",
      "cohort [83 53 70 45 44 39 22 80 10  0 18 30 73 33 90  4 76 77 12 31 55 88 26 42\n",
      " 69 15 40 96  9 72 11 47 85 28 93  5 66 65 35 16 49 34  7 95 27 19 81 25\n",
      " 62 13 24  3 17 38  8 78  6 64 36 89 56 99 54 43 50 67 46 68 61 97 79 41\n",
      " 58 48 98 57 75 32 94 59 63 84 37 29  1 52 21  2 23 87 91 74 86 82 20 60\n",
      " 71 14 92 51]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "simulator = RMABSimulator(all_population_size, all_features, all_transitions,\n",
    "            n_arms, volunteers_per_arm, episode_len, n_epochs, n_episodes, budget, discount,number_states=n_states, reward_style='match',match_probability_list=match_probabilities,TIME_PER_RUN=TIME_PER_RUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results['parameters'] = {'seed'      : seed,\n",
    "        'n_arms'    : n_arms,\n",
    "        'volunteers_per_arm': volunteers_per_arm, \n",
    "        'budget'    : budget,\n",
    "        'discount'  : discount, \n",
    "        'alpha'     : alpha, \n",
    "        'n_episodes': n_episodes, \n",
    "        'episode_len': episode_len, \n",
    "        'n_epochs'  : n_epochs, \n",
    "        'lamb': lamb,\n",
    "        'time_per_run': TIME_PER_RUN} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance 0, ep 1\n",
      "instance 0, ep 2\n",
      "instance 0, ep 3\n",
      "instance 0, ep 4\n",
      "instance 0, ep 5\n",
      "instance 0, ep 6\n",
      "instance 0, ep 7\n",
      "instance 0, ep 8\n",
      "instance 0, ep 9\n",
      "instance 0, ep 10\n",
      "instance 0, ep 11\n",
      "instance 0, ep 12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m policy \u001b[39m=\u001b[39m whittle_policy\n\u001b[1;32m      2\u001b[0m name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlinear_whittle\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m whittle_reward, whittle_active_rate \u001b[39m=\u001b[39m run_heterogenous_policy(simulator, n_episodes, n_epochs, discount,policy,seed,lamb\u001b[39m=\u001b[39;49mlamb,should_train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,test_T\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m time_whittle \u001b[39m=\u001b[39m simulator\u001b[39m.\u001b[39mtime_taken    \n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39mmean(whittle_reward) \u001b[39m+\u001b[39m whittle_active_rate\u001b[39m*\u001b[39mlamb\u001b[39m*\u001b[39mn_arms\u001b[39m*\u001b[39mvolunteers_per_arm)\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/omniscient_policies.py:783\u001b[0m, in \u001b[0;36mrun_heterogenous_policy\u001b[0;34m(env, n_episodes, n_epochs, discount, policy, seed, per_epoch_function, lamb, get_memory, should_train, test_T)\u001b[0m\n\u001b[1;32m    780\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mobserve()\n\u001b[1;32m    782\u001b[0m action,memory \u001b[39m=\u001b[39m policy(env,state,budget,lamb,memory,per_epoch_results)\n\u001b[0;32m--> 783\u001b[0m next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    786\u001b[0m \u001b[39mif\u001b[39;00m done \u001b[39mand\u001b[39;00m t\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m \u001b[39m<\u001b[39m T: env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m should_train:\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/simulator.py:173\u001b[0m, in \u001b[0;36mRMABSimulator.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    171\u001b[0m         idx \u001b[39m=\u001b[39m i\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvolunteers_per_arm \u001b[39m+\u001b[39m j\n\u001b[1;32m    172\u001b[0m         prob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransitions[i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstates[idx], action[idx], :]\n\u001b[0;32m--> 173\u001b[0m         next_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(a\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumber_states, p\u001b[39m=\u001b[39;49mprob)\n\u001b[1;32m    174\u001b[0m         next_states[idx] \u001b[39m=\u001b[39m next_state\n\u001b[1;32m    176\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext \u001b[39m=\u001b[39m generate_random_context(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_dim)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "policy = whittle_policy\n",
    "name = \"linear_whittle\"\n",
    "whittle_match, whittle_active_rate = run_heterogenous_policy(simulator, n_episodes, n_epochs, discount,policy,seed,lamb=lamb,should_train=True,test_T=1000)\n",
    "time_whittle = simulator.time_taken\n",
    "whittle_discounted_reward = get_discounted_reward(whittle_match,whittle_active_rate,discount,lamb)\n",
    "\n",
    "print(whittle_discounted_reward)\n",
    "\n",
    "results['{}_reward'.format(name)] = whittle_discounted_reward\n",
    "results['{}_match'.format(name)] = np.mean(whittle_match) \n",
    "results['{}_active'.format(name)] = np.mean(whittle_active_rate)\n",
    "results['{}_time'.format(name)] = time_whittle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5000768  0.4630837  0.56196517 ... 0.41113216 0.4744327  0.5158847 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m policy \u001b[39m=\u001b[39m full_mcts_policy \n\u001b[1;32m      2\u001b[0m name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmcts\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m mcts_reward, mcts_active_rate,memory \u001b[39m=\u001b[39m run_heterogenous_policy(simulator, n_episodes, n_epochs, discount,policy,seed,lamb\u001b[39m=\u001b[39;49mlamb,get_memory\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,should_train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,test_T\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m mcts_time \u001b[39m=\u001b[39m simulator\u001b[39m.\u001b[39mtime_taken\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39mmean(mcts_reward) \u001b[39m+\u001b[39m mcts_active_rate\u001b[39m*\u001b[39mlamb\u001b[39m*\u001b[39mn_arms\u001b[39m*\u001b[39mvolunteers_per_arm)\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/omniscient_policies.py:782\u001b[0m, in \u001b[0;36mrun_heterogenous_policy\u001b[0;34m(env, n_episodes, n_epochs, discount, policy, seed, per_epoch_function, lamb, get_memory, should_train, test_T)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, T):\n\u001b[1;32m    780\u001b[0m     state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mobserve()\n\u001b[0;32m--> 782\u001b[0m     action,memory \u001b[39m=\u001b[39m policy(env,state,budget,lamb,memory,per_epoch_results)\n\u001b[1;32m    783\u001b[0m     next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m    786\u001b[0m     \u001b[39mif\u001b[39;00m done \u001b[39mand\u001b[39;00m t\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m \u001b[39m<\u001b[39m T: env\u001b[39m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/mcts_policies.py:722\u001b[0m, in \u001b[0;36mfull_mcts_policy\u001b[0;34m(env, state, budget, lamb, memory, per_epoch_results, timeLimit)\u001b[0m\n\u001b[1;32m    720\u001b[0m     value_with_pull \u001b[39m=\u001b[39m get_total_value(env,all_match_probs,best_group_arms,state,new_group_index,value_network,lamb,num_future_samples\u001b[39m=\u001b[39m\u001b[39m25\u001b[39m)\n\u001b[1;32m    721\u001b[0m     upper_bound \u001b[39m=\u001b[39m current_value \u001b[39m+\u001b[39m (value_with_pull\u001b[39m-\u001b[39mcurrent_value)\u001b[39m*\u001b[39m(budget\u001b[39m-\u001b[39mk)\n\u001b[0;32m--> 722\u001b[0m     ucb_past \u001b[39m=\u001b[39m scores_current_combo[arm] \u001b[39m+\u001b[39m exploration_const\u001b[39m*\u001b[39mpolicy_by_group[arm][new_group_index[arm]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m/\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m+\u001b[39mn)\n\u001b[1;32m    723\u001b[0m     UCB_by_arm[arm] \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(upper_bound,ucb_past)\n\u001b[1;32m    724\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mmax\u001b[39m(UCB_by_arm\u001b[39m.\u001b[39mvalues()) \u001b[39m<\u001b[39m best_score \u001b[39mand\u001b[39;00m num_epochs \u001b[39m>\u001b[39m \u001b[39m500\u001b[39m:\n",
      "File \u001b[0;32m~/projects/food_rescue_rmab/rmab/mcts_policies.py:497\u001b[0m, in \u001b[0;36mget_total_value\u001b[0;34m(env, all_match_probs, best_group_arms, state, group_indices, value_network, lamb, num_future_samples)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(state)):\n\u001b[1;32m    495\u001b[0m     probs\u001b[39m.\u001b[39mappend(env\u001b[39m.\u001b[39mtransitions[i\u001b[39m/\u001b[39m\u001b[39m/\u001b[39menv\u001b[39m.\u001b[39mvolunteers_per_arm, state[i], action[i], \u001b[39m1\u001b[39m])\n\u001b[0;32m--> 497\u001b[0m samples \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrandom((num_future_samples,\u001b[39mlen\u001b[39;49m(state))) \u001b[39m<\u001b[39;49m probs \n\u001b[1;32m    498\u001b[0m samples \u001b[39m=\u001b[39m samples\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[1;32m    499\u001b[0m future_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(value_network(torch\u001b[39m.\u001b[39mTensor(samples)))\u001b[39m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "simulator.mcts_test_iterations = 10\n",
    "simulator.mcts_train_iterations = 10\n",
    "\n",
    "policy = full_mcts_policy \n",
    "name = \"mcts\"\n",
    "mcts_match, mcts_active_rate,memory = run_heterogenous_policy(simulator, n_episodes, n_epochs, discount,policy,seed,lamb=lamb,get_memory=True,should_train=True,test_T=1000)\n",
    "time_mcts = simulator.time_taken\n",
    "mcts_discounted_reward = get_discounted_reward(mcts_match,mcts_active_rate,discount,lamb)\n",
    "\n",
    "print(mcts_discounted_reward)\n",
    "\n",
    "results['{}_reward'.format(name)] = mcts_discounted_reward\n",
    "results['{}_match'.format(name)] = np.mean(mcts_match) \n",
    "results['{}_active'.format(name)] = np.mean(mcts_active_rate)\n",
    "results['{}_time'.format(name)] = time_mcts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = get_save_path('real_data',save_name,seed,use_date=save_with_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_duplicate_results('real_data',\"\",results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(results,open('../results/'+save_path,'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
